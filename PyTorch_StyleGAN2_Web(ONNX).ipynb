{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "PyTorch_StyleGAN2_Web(ONNX).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFjVwUap4Q0I",
        "outputId": "04990194-4a14-4471-df7d-0393cc35573f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "32JtLa3wJGNY",
        "outputId": "a11d7a55-ea5d-411d-8219-52a659614a02"
      },
      "source": [
        "'''%データセットのダウンロード\n",
        "!pip install gdown\n",
        "\n",
        "import gdown\n",
        "file_id = \"1-EyM2kIj24P6DtT-swZP8DLyBAPU1PkU\"\n",
        "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "output = \"dataset.zip\"\n",
        "gdown.download(url, output, quiet=False)\n",
        "!unzip \"dataset.zip\"\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'%データセットのダウンロード\\n!pip install gdown\\n\\nimport gdown\\nfile_id = \"1-EyM2kIj24P6DtT-swZP8DLyBAPU1PkU\"\\nurl = f\"https://drive.google.com/uc?id={file_id}\"\\noutput = \"dataset.zip\"\\ngdown.download(url, output, quiet=False)\\n!unzip \"dataset.zip\"\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8o_3yBZqJwOc",
        "outputId": "fa28442c-85a2-45bf-f3a1-46d85c8a536c"
      },
      "source": [
        "#import libraries\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda, Compose\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "print(torch.__version__)\n",
        "torch.manual_seed(0)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.8.1+cu101\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f7b0965bb30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lO_Cl9QBJyU0",
        "outputId": "9d047c74-ad28-4bb5-a95d-9e94d28b8989"
      },
      "source": [
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using {} device\".format(device))\n",
        "LReLU_alpha = 0.2\n",
        "mapping_lamda = 0.01\n",
        "\n",
        "#https://github.com/yuuho/stylegans-pytorch/blob/master/network/stylegan2.py　を参考に\n",
        "from torch.nn import functional as F\n",
        "class modulated_conv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros'):\n",
        "        super(modulated_conv2d, self).__init__()\n",
        "        self.padding, self.stride = padding, stride\n",
        "\n",
        "        self.weight = nn.Parameter(torch.randn(out_channels, in_channels, kernel_size, kernel_size))\n",
        "        torch.nn.init.normal_(self.weight.data, mean=0.0, std=1.0)\n",
        "        #self.weight = nn.Parameter(torch.zeros(out_channels, in_channels, kernel_size, kernel_size))\n",
        "        self.bias = nn.Parameter(torch.zeros(1,out_channels,1,1))\n",
        "        self.weight_scaler = 1 / (in_channels * kernel_size*kernel_size)**0.5\n",
        "\n",
        "    def forward(self,x,style,shape = None,web = True,style_2 = None):\n",
        "        oC, iC, kH, kW = self.weight.shape\n",
        "        if shape is None:\n",
        "            N, iC, H, W = x.shape\n",
        "        else:\n",
        "            N, iC, H, W = shape\n",
        "\n",
        "        if not web:\n",
        "            modulated_weight = self.weight.view(1,oC,iC,kH,kW) * self.weight_scaler \n",
        "            #modulated_weight = modulated_weight.repeat(N,1,1,1,1)\n",
        "            x = x * style.view(N,iC,1,1)\n",
        "            x = F.conv2d(x.view(1,N*iC,H,W), modulated_weight.view(N*oC,iC,kH,kW),\n",
        "                    padding=self.padding, stride=self.stride, groups=N).view(N,oC,H,W)\n",
        "            modulated_weight = modulated_weight * style_2.view(N,1,iC,1,1)\n",
        "            demod_norm = 1 / torch.sqrt((modulated_weight * modulated_weight).sum([2,3,4])  + 1e-8)\n",
        "            out = x * demod_norm.view(N, oC, 1, 1) + self.bias   \n",
        "\n",
        "        else:\n",
        "            modulated_weight = self.weight_scaler*self.weight.view(1,oC,iC,kH,kW) \\\n",
        "                                    * style.view(N,1,iC,1,1) \n",
        "\n",
        "            demod_norm = 1 / torch.sqrt((modulated_weight * modulated_weight).sum([2,3,4]) + 1e-8) # (N, oC)\n",
        "            demodulated_weight = modulated_weight * demod_norm.view(N, oC, 1, 1, 1) \n",
        "            #demodulated_weight = modulated_weight\n",
        "            out = F.conv2d(x.view(1,N*iC,H,W), demodulated_weight.view(N*oC,iC,kH,kW),\n",
        "                    padding=self.padding, stride=self.stride, groups=N).view(N,oC,H,W) + self.bias\n",
        "            \n",
        "        return out\n",
        "\n",
        "def alternative_Upsample(image,input_size):\n",
        "    batches, channels, h, w = input_size\n",
        "\n",
        "    x = image.view(batches, channels, h * w, 1)\n",
        "    x = torch.cat((x,x),3)\n",
        "    x = x.view(batches, channels, h, w * 2)\n",
        "    x = torch.cat((x,x),3)\n",
        "    x = x.view(batches, channels, h * 2, w * 2)\n",
        "    return x\n",
        "# Define model\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        dimensions = [256,256,256,256,128,128,128,64,64,64,32,32]\n",
        "        self.learning_const = torch.randn((1,dimensions[1],4,4),requires_grad=True, device = device)\n",
        "        self.mapping_network = self.generate_mapping_network(dimensions[0])\n",
        "\n",
        "        self.affine_4_0 = self.make_latent_to_style(dimensions[0],dimensions[1])\n",
        "        self.block_4_0 = self.generate_block(4,dimensions[1],dimensions[2])\n",
        "        self.noise_4_0 = torch.zeros((1,),requires_grad=True, device = device)\n",
        "        self.affine_4_1 = self.make_latent_to_style(dimensions[0],dimensions[2])\n",
        "        self.block_4_1 = self.generate_block(4,dimensions[2],dimensions[3])\n",
        "        self.noise_4_1 = torch.zeros((1,),requires_grad=True, device = device)\n",
        "        self.to_rgb_4 = nn.Conv2d(dimensions[3],3,kernel_size=1,stride=1,padding=0)\n",
        "\n",
        "        self.affine_8_0 = self.make_latent_to_style(dimensions[0],dimensions[3])\n",
        "        self.block_8_0 = self.generate_block(8,dimensions[3],dimensions[4],True)\n",
        "        self.noise_8_0 = torch.zeros((1,),requires_grad=True, device = device)\n",
        "        self.affine_8_1 = self.make_latent_to_style(dimensions[0],dimensions[4])\n",
        "        self.block_8_1 = self.generate_block(8,dimensions[4],dimensions[5])\n",
        "        self.noise_8_1 = torch.zeros((1,),requires_grad=True, device = device)\n",
        "        self.to_rgb_8 = nn.Conv2d(dimensions[5],3,kernel_size=1,stride=1,padding=0)\n",
        "\n",
        "        self.affine_16_0 = self.make_latent_to_style(dimensions[0],dimensions[5])\n",
        "        self.block_16_0 = self.generate_block(16,dimensions[5],dimensions[6],True)\n",
        "        self.noise_16_0 = torch.zeros((1,),requires_grad=True, device = device)\n",
        "        self.affine_16_1 = self.make_latent_to_style(dimensions[0],dimensions[6])\n",
        "        self.block_16_1 = self.generate_block(16,dimensions[6],dimensions[7])\n",
        "        self.noise_16_1 = torch.zeros((1,),requires_grad=True, device = device)\n",
        "        self.to_rgb_16 = nn.Conv2d(dimensions[7],3,kernel_size=1,stride=1,padding=0)\n",
        "\n",
        "        self.affine_32_0 = self.make_latent_to_style(dimensions[0],dimensions[7])\n",
        "        self.block_32_0 = self.generate_block(32,dimensions[7],dimensions[8],True)\n",
        "        self.noise_32_0 = torch.zeros((1,),requires_grad=True, device = device)\n",
        "        self.affine_32_1 = self.make_latent_to_style(dimensions[0],dimensions[8])\n",
        "        self.block_32_1 = self.generate_block(32,dimensions[8],dimensions[9])\n",
        "        self.noise_32_1 = torch.zeros((1,),requires_grad=True, device = device)\n",
        "        self.to_rgb_32 = nn.Conv2d(dimensions[9],3,kernel_size=1,stride=1,padding=0)\n",
        "\n",
        "        self.affine_64_0 = self.make_latent_to_style(dimensions[0],dimensions[9])\n",
        "        self.block_64_0 = self.generate_block(64,dimensions[9],dimensions[10],True)\n",
        "        self.noise_64_0 = torch.zeros((1,),requires_grad=True, device = device)\n",
        "        self.affine_64_1 = self.make_latent_to_style(dimensions[0],dimensions[10])\n",
        "        self.block_64_1 = self.generate_block(64,dimensions[10],dimensions[11])\n",
        "        self.noise_64_1 = torch.zeros((1,),requires_grad=True, device = device)\n",
        "        self.to_rgb_64 = nn.Conv2d(dimensions[11],3,kernel_size=1,stride=1,padding=0)\n",
        "        for i in range(5):\n",
        "            nn.init.normal_(getattr(self, 'to_rgb_{0}'.format(2 ** (i + 2))).weight, 0.0, 1.0 / getattr(self, 'to_rgb_{0}'.format(2 ** (i + 2))).weight.shape[1] ** 0.5)\n",
        "            nn.init.constant_(getattr(self, 'to_rgb_{0}'.format(2 ** (i + 2))).bias, 0.0)\n",
        "            \n",
        "            for j in range(2):\n",
        "                nn.init.normal_(getattr(self, 'affine_{0}_{1}'.format(2 ** (i + 2),j)).weight, 0.0, 1.0)\n",
        "                nn.init.constant_(getattr(self, 'affine_{0}_{1}'.format(2 ** (i + 2),j)).bias, 1.0)         \n",
        "\n",
        "    def generate_mapping_network(self,dimension = 512,number_of_layer = 1):\n",
        "       mapping_network = nn.Sequential()\n",
        "       for i in range(number_of_layer):\n",
        "           mapping_network.add_module('mapping_fc{0}'.format(i), nn.Linear(dimension,dimension))\n",
        "           mapping_network.add_module('mapping_lrelu{0}'.format(i), nn.LeakyReLU())\n",
        "           nn.init.normal_(getattr(mapping_network, 'mapping_fc{0}'.format(i)).weight, 0.0, 1.0)\n",
        "           nn.init.constant_(getattr(mapping_network, 'mapping_fc{0}'.format(i)).bias, 0)\n",
        "       return mapping_network\n",
        "\n",
        "    def generate_block(self,image_size = 4,in_dimension = 512, out_dimension = 512,upscaling = False):\n",
        "        return modulated_conv2d(in_dimension,out_dimension,kernel_size=3,stride=1,padding=1)\n",
        "        #return torch.nn.Conv2d(in_dimension,out_dimension,kernel_size=3,stride=1,padding=1)\n",
        "    def make_latent_to_style(self,latent_dimension = 512, target_dimension = 512):\n",
        "\n",
        "        return nn.Linear(latent_dimension, target_dimension)\n",
        "    def forward(self, z, stage = 1 ,alpha = 0, batches = 1,web = False):\n",
        "        style = self.mapping_network(z) * mapping_lamda\n",
        "        if web:\n",
        "            #mapping_network_2 = self.mapping_network_2(z)\n",
        "            style_2 = self.mapping_network(z) * mapping_lamda\n",
        "\n",
        "        x = self.learning_const.repeat(int(batches),1,1,1)\n",
        "        for i in range(stage):\n",
        "            if i != 0:\n",
        "                if web:\n",
        "                    x = alternative_Upsample(x,(1,-1,2 ** (i + 1),2 ** (i + 1)))\n",
        "                else:\n",
        "                    x = torch.nn.functional.interpolate(x,scale_factor=2, mode='nearest')\n",
        "            for j in range(2):\n",
        "                if not (i == 0 and j == 0):\n",
        "                    affined_style = getattr(self, 'affine_{0}_{1}'.format(2 ** (i + 2),j))(style)\n",
        "                    if web:\n",
        "                        affined_style_2 = getattr(self, 'affine_{0}_{1}'.format(2 ** (i + 2),j))(style_2)\n",
        "                    if web:\n",
        "                        x = getattr(self, 'block_{0}_{1}'.format(2 ** (i + 2),j))(x,affined_style,shape = (1,-1,2 ** (i + 2),2 ** (i + 2)),style_2 = affined_style_2,web = True)\n",
        "                    else:\n",
        "                        x = getattr(self, 'block_{0}_{1}'.format(2 ** (i + 2),j))(x,affined_style,shape = (1,-1,2 ** (i + 2),2 ** (i + 2)),web = False)\n",
        "                    x = F.leaky_relu(x,LReLU_alpha)\n",
        "                    #x = x + torch.randn(x.shape, device = device) *  getattr(self, 'noise_{0}_{1}'.format(2 ** (i + 2),j))\n",
        "\n",
        "        x_out = getattr(self, 'to_rgb_{0}'.format(2 ** (stage + 1)))(x)\n",
        "\n",
        "        if alpha != 0:\n",
        "            if web:\n",
        "                x = alternative_Upsample(x,(1,-1,2 ** (stage + 1),2 ** (stage + 1)))\n",
        "            else:\n",
        "                x = torch.nn.functional.interpolate(x,scale_factor=2, mode='nearest')\n",
        "            for j in range(2):\n",
        "                affined_style = getattr(self, 'affine_{0}_{1}'.format(2 ** (stage + 2),j))(style)\n",
        "                x = getattr(self, 'block_{0}_{1}'.format(2 ** (stage + 2),j))(x,affined_style,shape = (1,-1,2 ** (stage + 1),2 ** (stage + 1)))\n",
        "                x = F.leaky_relu(x,LReLU_alpha)\n",
        "                #x = x + torch.randn(x.shape, device = device) * getattr(self, 'noise_{0}_{1}'.format(2 ** (i + 2),j))\n",
        "            x = getattr(self, 'to_rgb_{0}'.format(2 ** (stage + 2)))(x)\n",
        "            if web:\n",
        "                x_out = alpha * x + alternative_Upsample(x_out,(1,-1,2 ** (stage + 1),2 ** (stage + 1))) * (1 - alpha)\n",
        "            else:\n",
        "                x_out = alpha * x + torch.nn.functional.interpolate(x,scale_factor=2, mode='nearest') * (1 - alpha)\n",
        "            \n",
        "        return x_out"
      ],
      "execution_count": 568,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dl4fVrUadKk4"
      },
      "source": [
        "# Define model\n",
        "from torch.autograd import Variable\n",
        "from torch import autograd\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        dimensions = [256,256,256,128,128,128,64,64,64,32,32,32]\n",
        "\n",
        "        self.from_rgb_4 = nn.Conv2d(3,dimensions[2],kernel_size=1,stride=1,padding=0)\n",
        "        self.block_4_0 = self.generate_block(4,dimensions[2],dimensions[1])\n",
        "        self.block_4_1 = self.generate_block(4,dimensions[1],dimensions[0])\n",
        "\n",
        "        self.from_rgb_8 = nn.Conv2d(3,dimensions[4],kernel_size=1,stride=1,padding=0)\n",
        "        self.block_8_0 = self.generate_block(8,dimensions[4],dimensions[3])\n",
        "        self.block_8_1 = self.generate_block(8,dimensions[3],dimensions[2])\n",
        "\n",
        "        self.from_rgb_16 = nn.Conv2d(3,dimensions[6],kernel_size=1,stride=1,padding=0)\n",
        "        self.block_16_0 = self.generate_block(16,dimensions[6],dimensions[5])\n",
        "        self.block_16_1 = self.generate_block(16,dimensions[5],dimensions[4])\n",
        "\n",
        "        self.from_rgb_32 = nn.Conv2d(3,dimensions[8],kernel_size=1,stride=1,padding=0)\n",
        "        self.block_32_0 = self.generate_block(32,dimensions[8],dimensions[7])\n",
        "        self.block_32_1 = self.generate_block(32,dimensions[7],dimensions[6])\n",
        "\n",
        "        self.from_rgb_64 = nn.Conv2d(3,dimensions[10],kernel_size=1,stride=1,padding=0)\n",
        "        self.block_64_0 = self.generate_block(64,dimensions[10],dimensions[9])\n",
        "        self.block_64_1 = self.generate_block(64,dimensions[9],dimensions[8])\n",
        "\n",
        "        self.final_conv = nn.Conv2d(dimensions[0],dimensions[0],kernel_size=4,stride=1,padding=0)\n",
        "        self.linear = nn.Linear(dimensions[0],1)\n",
        "        print(self.final_conv.weight)\n",
        "        nn.init.normal_(self.final_conv.weight, 0.0, 1.0 / (4*4**dimensions[0]) ** 0.5)\n",
        "        nn.init.constant_(self.final_conv.bias, 0)\n",
        "        nn.init.normal_(self.linear.weight, 0.0, 1.0)\n",
        "        nn.init.constant_(self.linear.bias, 0)\n",
        "        for i in range(5):\n",
        "            nn.init.normal_(getattr(self, 'from_rgb_{0}'.format(2 ** (i + 2))).weight, 0.0, 1.0 / getattr(self, 'from_rgb_{0}'.format(2 ** (i + 2))).weight.shape[1] ** 0.5)\n",
        "            nn.init.constant_(getattr(self, 'from_rgb_{0}'.format(2 ** (i + 2))).bias, 0.0)\n",
        "\n",
        "    def generate_block(self,image_size = 4,in_dimension = 512, out_dimension = 512):\n",
        "        block = nn.Sequential()\n",
        "\n",
        "        block.add_module('Conv', nn.Conv2d(in_dimension,out_dimension,kernel_size=3,stride=1,padding=1))\n",
        "        block.add_module('relu', nn.LeakyReLU(LReLU_alpha))\n",
        "        nn.init.normal_(block.Conv.weight, 0.0, 1.0)\n",
        "        nn.init.constant_(block.Conv.bias, 0)\n",
        "\n",
        "        return block\n",
        "    #https://github.com/Zeleni9/pytorch-wgan/blob/master/models/wgan_gradient_penalty.py を改変\n",
        "    def calculate_gradient_penalty(self, real_images, fake_images,batch_size,stage,alpha):\n",
        "        eta = torch.FloatTensor(batch_size,1,1,1).uniform_(0,1).to(device)\n",
        "        eta = eta.expand(batch_size, real_images.size(1), real_images.size(2), real_images.size(3))\n",
        "\n",
        "        interpolated = eta * real_images + ((1 - eta) * fake_images)\n",
        "\n",
        "        # define it to calculate gradient\n",
        "        interpolated = Variable(interpolated, requires_grad=True)\n",
        "        # calculate probability of interpolated examples\n",
        "        prob_interpolated = self(interpolated,stage,alpha)\n",
        "        # calculate gradients of probabilities with respect to examples\n",
        "        gradients = autograd.grad(outputs=prob_interpolated, inputs=interpolated,\n",
        "                              grad_outputs=torch.ones(\n",
        "                                  prob_interpolated.size()).to(device),\n",
        "                              create_graph=True, retain_graph=True)[0]\n",
        "        grad_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
        "        return grad_penalty\n",
        "    \n",
        "    def forward(self, image, stage = 1 ,alpha = 0, batches = 1):\n",
        "        if alpha != 0:\n",
        "            x = getattr(self, 'from_rgb_{0}'.format(2 ** (stage + 2)))(image)\n",
        "            for j in range(2):\n",
        "                x = getattr(self, 'block_{0}_{1}'.format(2 ** (stage + 2),j))(x)\n",
        "            x = torch.nn.functional.interpolate(x,scale_factor=0.5, mode='nearest')\n",
        "            x2 = getattr(self, 'from_rgb_{0}'.format(2 ** (stage + 1)))(torch.nn.functional.interpolate(image,scale_factor=0.5, mode='nearest'))\n",
        "            x = x * alpha + x2 * (1 - alpha)\n",
        "        else:\n",
        "            x = getattr(self, 'from_rgb_{0}'.format(2 ** (stage + 1)))(image)\n",
        "\n",
        "        for i in range(stage):\n",
        "            for j in range(2):\n",
        "                if not (i == stage - 1 and j == 1):\n",
        "                    x = getattr(self, 'block_{0}_{1}'.format(2 ** (stage + 1 - i),j))(x)\n",
        "            if i != stage - 1:\n",
        "                x = torch.nn.functional.interpolate(x,scale_factor=0.5, mode='nearest')\n",
        "\n",
        "        x = self.final_conv(x)\n",
        "        \n",
        "        x = nn.LeakyReLU(LReLU_alpha)(x)\n",
        "        x = nn.Flatten()(x)\n",
        "        return  self.linear(x)"
      ],
      "execution_count": 569,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evars0QQOPuM"
      },
      "source": [
        "class trainer():\n",
        "    def __init__(self,learning_rate = 0.0002):\n",
        "        self.alpha = 0\n",
        "        self.stage = 0\n",
        "        self.MAX_STAGE = 4\n",
        "        self.BATCH_SIZE = 32\n",
        "        self.n_critic = 1\n",
        "        self.gp_lamda = 1\n",
        "        self.g = Generator().to(device)\n",
        "        self.d = Discriminator().to(device)\n",
        "        self.G_optimizer = torch.optim.Adam(self.g.parameters(), lr=learning_rate)\n",
        "        self.D_optimizer = torch.optim.Adam(self.d.parameters(), lr=learning_rate)\n",
        "        self.NUM_EPOCH_NO_ALPHA = 50\n",
        "        self.NUM_EPOCH_WITH_ALPHA = 50\n",
        "        self.z_dimension = 256\n",
        "    def train(self):\n",
        "        MAX_STAGE = self.MAX_STAGE\n",
        "        BATCH_SIZE = self.BATCH_SIZE\n",
        "        n_critic = self.n_critic\n",
        "        gp_lamda = self.gp_lamda\n",
        "        g = self.g\n",
        "        d = self.d\n",
        "        G_optimizer = self.G_optimizer\n",
        "        D_optimizer = self.D_optimizer\n",
        "        NUM_EPOCH_NO_ALPHA = self.NUM_EPOCH_NO_ALPHA\n",
        "        NUM_EPOCH_WITH_ALPHA = self.NUM_EPOCH_WITH_ALPHA\n",
        "        z_dimension = self.z_dimension\n",
        "        training_data = datasets.CIFAR10(\n",
        "            root=\"data\",\n",
        "            train=True,\n",
        "            download=True,\n",
        "            transform=ToTensor(),\n",
        "        )\n",
        "        train_dataloader = DataLoader(training_data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "        while True:\n",
        "            for epoch in range(NUM_EPOCH_NO_ALPHA + NUM_EPOCH_WITH_ALPHA):\n",
        "                number_of_batch = 0\n",
        "                for X, _ in train_dataloader:\n",
        "                    X = X * 2 - 1\n",
        "                    \n",
        "                    alpha = 0\n",
        "                    if int(self.stage) % 2 == 1:\n",
        "                        self.stage += 1 / (len(train_dataloader) * NUM_EPOCH_WITH_ALPHA) \n",
        "                    elif math.ceil(self.stage / 2) < self.MAX_STAGE:\n",
        "                        self.stage += 1 / (len(train_dataloader) * NUM_EPOCH_NO_ALPHA)\n",
        "                    stage = math.ceil(self.stage / 2)    \n",
        "                    if int(self.stage) % 2 == 1:\n",
        "                        alpha = self.stage - int(self.stage)\n",
        "                    \n",
        "                    for _ in range(n_critic):\n",
        "                        z = torch.randn((BATCH_SIZE,z_dimension),device = device)\n",
        "                        generated = g(z, stage = stage ,alpha = alpha, batches = BATCH_SIZE)\n",
        "                        X = X.to(device)\n",
        "                        X = torch.nn.functional.interpolate(X,size=generated.size(2), mode='Nearest')\n",
        "                        y_real = d(X, stage = stage ,alpha = alpha, batches = BATCH_SIZE)\n",
        "                        y_fake = d(generated, stage = stage ,alpha = alpha, batches = BATCH_SIZE)\n",
        "                        d_loss = torch.mean(y_fake) - torch.mean(y_real) + d.calculate_gradient_penalty(X,generated,BATCH_SIZE,stage,alpha) * gp_lamda\n",
        "                        Wasserstein_loss =  (torch.mean(y_fake) - torch.mean(y_real)).data\n",
        "\n",
        "                        D_optimizer.zero_grad()\n",
        "                        d_loss.backward()\n",
        "                        D_optimizer.step()\n",
        "                    g_loss = 'undefined'\n",
        "                    if Wasserstein_loss <= 0:\n",
        "                        z = torch.randn((BATCH_SIZE,z_dimension),device = device)\n",
        "                        generated = g(z, stage = stage ,alpha = alpha, batches = BATCH_SIZE)\n",
        "                        y_fake = d(generated, stage = stage ,alpha = alpha, batches = BATCH_SIZE)\n",
        "                        g_loss = -torch.mean(y_fake)\n",
        "                        G_optimizer.zero_grad()\n",
        "                        g_loss.backward()\n",
        "                        G_optimizer.step()\n",
        "\n",
        "                    \n",
        "\n",
        "                    if number_of_batch % 100 == 0:\n",
        "                        print('epoch:{}, batch:{},stage:{},alpha{},g_loss:{}, d_loss:{}, Wasserstein Loss:{}'.format(epoch,number_of_batch,stage,alpha,g_loss,d_loss,Wasserstein_loss))\n",
        "                        show_image(generated)\n",
        "                        show_image(X)\n",
        "                        torch.save(self, '/content/drive/MyDrive/StyleGAN2/model.pth')\n",
        "                    number_of_batch += 1"
      ],
      "execution_count": 570,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMy29H2BzU5w",
        "outputId": "77e55f2b-b817-4742-ab8e-fa0b3dca5cd4"
      },
      "source": [
        "try:\n",
        "    train = torch.load('/content/drive/MyDrive/StyleGAN2/model.pth')\n",
        "except:\n",
        "    train = trainer()\n",
        "    print('load failed')\n",
        "\n",
        "dummy_input = torch.randn(1, train.z_dimension, device=device)\n",
        "train.g.learning_const.requires_grad = False\n",
        "for i in range(5):  \n",
        "    for j in range(2):\n",
        "        getattr(train.g, 'noise_{0}_{1}'.format(2 ** (i + 2),j)).requires_grad = False\n",
        "        getattr(train.g, 'noise_{0}_{1}'.format(2 ** (i + 2),j)).requires_grad = False\n",
        "stage = torch.tensor(2, dtype=torch.int)\n",
        "alpha = torch.tensor(0, dtype=torch.int)\n",
        "batches = torch.tensor(1, dtype=torch.int)\n",
        "web = torch.tensor(True, dtype=torch.bool)\n",
        "torch.onnx.export(train.g, (dummy_input,stage,alpha,batches,web), 'generator.onnx', opset_version= 9)"
      ],
      "execution_count": 571,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:129: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:133: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:143: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:145: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:136: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:154: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBqbMLLl5Ip8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "53d0502b-845d-4c71-caa7-1fdb4fb53cc2"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('generator.onnx')"
      ],
      "execution_count": 572,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_cada4479-e234-4c06-9404-9d16f49a9acc\", \"generator.onnx\", 6919324)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMqPXkseJ6xc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dac4a3b-d09f-4ca5-e40b-c3c195ff0e5a"
      },
      "source": [
        "#train = torch.load('/content/drive/MyDrive/StyleGAN2/model.pth')\n",
        "train = trainer()\n",
        "z = torch.zeros(1, train.z_dimension, device=device)\n",
        "train.g(z,web=True)"
      ],
      "execution_count": 573,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[[[-1.0603e-02, -5.1772e-03,  1.5233e-02, -4.2289e-03],\n",
            "          [-1.0555e-03, -1.6291e-03, -6.8371e-03,  7.7916e-03],\n",
            "          [ 6.0181e-03, -4.3165e-03,  1.5622e-02,  4.0209e-03],\n",
            "          [ 5.1694e-03,  1.4983e-02, -1.1765e-02,  1.3383e-02]],\n",
            "\n",
            "         [[-5.1310e-03,  1.4831e-02,  5.9941e-03, -1.0257e-02],\n",
            "          [-7.3600e-03, -1.1799e-02, -1.2235e-02,  5.0932e-03],\n",
            "          [ 1.4311e-02,  3.1430e-03, -2.5331e-03,  1.5005e-02],\n",
            "          [ 7.4245e-03, -7.2554e-03, -9.6138e-03,  2.1882e-03]],\n",
            "\n",
            "         [[ 9.1694e-03,  3.9166e-03, -1.2406e-02,  1.1120e-02],\n",
            "          [-7.6154e-03, -1.1288e-02, -8.9001e-03,  6.2036e-03],\n",
            "          [-1.2238e-02,  3.8078e-03, -1.3888e-02, -1.3146e-02],\n",
            "          [ 2.9443e-03, -1.1632e-02,  1.3879e-02, -7.3523e-04]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 8.2911e-03, -5.2284e-03, -6.5510e-04,  4.3978e-03],\n",
            "          [ 1.2231e-02,  4.0465e-03, -3.1562e-03, -1.9690e-03],\n",
            "          [ 6.2135e-03,  7.9574e-03, -6.7430e-03,  1.4901e-02],\n",
            "          [-9.3345e-03,  1.3775e-02, -1.0764e-02,  8.8526e-04]],\n",
            "\n",
            "         [[ 1.3024e-02,  8.0265e-03,  1.0473e-02, -5.8972e-04],\n",
            "          [ 6.4128e-03, -4.7108e-03,  1.2665e-02, -1.1584e-02],\n",
            "          [ 3.9602e-03,  1.3970e-02, -8.2646e-03, -6.9235e-03],\n",
            "          [ 1.2732e-02,  7.3853e-04, -1.7060e-03, -1.0581e-02]],\n",
            "\n",
            "         [[-3.0847e-03,  6.3913e-03, -6.8853e-03, -1.3580e-02],\n",
            "          [-6.5086e-03, -9.0092e-04,  4.9945e-03,  1.2016e-02],\n",
            "          [-9.0347e-03,  5.0579e-03,  1.5276e-02,  8.4646e-03],\n",
            "          [ 6.2902e-03, -1.9849e-03,  8.6445e-03, -3.3615e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 1.2619e-02,  1.3081e-02, -1.4079e-02, -1.1921e-02],\n",
            "          [-1.4766e-02,  1.1267e-02, -1.0825e-02, -6.8854e-03],\n",
            "          [-1.1303e-02, -1.3410e-02,  9.6418e-03, -2.8328e-03],\n",
            "          [ 1.3699e-02,  2.6299e-03,  1.0180e-02, -9.5079e-03]],\n",
            "\n",
            "         [[-9.3539e-03, -5.1048e-03, -1.1532e-02, -1.0859e-02],\n",
            "          [-1.0820e-02, -9.4883e-05, -1.0360e-02, -8.9462e-04],\n",
            "          [ 3.9105e-03, -1.1629e-03,  5.4959e-03,  1.4861e-02],\n",
            "          [ 1.0550e-02, -1.2427e-02, -1.4258e-02,  6.4425e-03]],\n",
            "\n",
            "         [[-4.6126e-03,  1.2551e-02,  4.4829e-04,  9.8844e-03],\n",
            "          [-6.6221e-03,  7.4775e-03, -1.3324e-02,  8.7579e-03],\n",
            "          [-8.9281e-03,  6.3170e-03, -5.8952e-03,  1.4311e-02],\n",
            "          [ 3.1568e-03,  3.2602e-03,  1.1487e-02, -1.2482e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.1295e-03, -1.0064e-02, -4.4783e-03, -2.5237e-03],\n",
            "          [-9.7275e-03, -9.7676e-03,  7.4682e-03,  7.0077e-03],\n",
            "          [ 9.9029e-04,  1.3081e-02,  9.4090e-03, -6.0219e-03],\n",
            "          [-6.8474e-03,  5.4362e-03,  2.1511e-03, -9.1280e-03]],\n",
            "\n",
            "         [[-6.7928e-03, -1.1701e-02, -2.9311e-03, -7.8748e-03],\n",
            "          [-8.7213e-03, -5.9805e-03,  2.7124e-03,  2.5413e-03],\n",
            "          [-4.5602e-03, -1.2871e-02, -1.5251e-02, -7.7402e-03],\n",
            "          [-9.2687e-03, -1.2015e-02, -1.1704e-02,  1.2491e-02]],\n",
            "\n",
            "         [[-8.8509e-03, -6.3106e-03, -7.4216e-03, -2.2563e-03],\n",
            "          [ 5.8403e-03,  3.1836e-03, -7.0509e-03, -1.2743e-02],\n",
            "          [-5.7134e-03,  7.5973e-03,  1.5977e-03,  2.8970e-05],\n",
            "          [ 5.7660e-03,  8.2741e-03,  1.4914e-04,  2.4714e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 9.3320e-03, -5.9098e-03,  1.4569e-02, -6.2235e-03],\n",
            "          [ 1.2622e-02,  1.5479e-02, -5.2359e-03, -1.1326e-02],\n",
            "          [ 1.5575e-02,  1.5052e-02, -9.9358e-04,  8.8236e-04],\n",
            "          [ 8.8126e-03, -5.8031e-03,  3.4257e-03, -1.0236e-03]],\n",
            "\n",
            "         [[-8.5318e-05,  1.0172e-02,  8.6092e-03, -1.4779e-02],\n",
            "          [-1.1227e-02, -4.5563e-03,  1.0167e-02, -9.3714e-03],\n",
            "          [ 4.7991e-03, -6.9903e-03,  1.3978e-02,  3.4668e-03],\n",
            "          [-1.4880e-02,  6.0980e-03,  6.5492e-03,  4.7231e-03]],\n",
            "\n",
            "         [[ 1.5545e-02,  6.4983e-03,  7.2126e-03, -8.5341e-03],\n",
            "          [ 3.4031e-03,  2.8053e-03, -1.5468e-03, -8.3674e-03],\n",
            "          [-9.0987e-03, -2.7962e-04, -6.9678e-03,  8.1061e-03],\n",
            "          [-1.2745e-04,  6.7714e-03, -1.4851e-02,  1.5059e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.1514e-02, -1.3480e-02, -4.9502e-03,  1.4294e-02],\n",
            "          [ 3.2973e-04, -2.3666e-04,  9.6547e-03, -1.1741e-02],\n",
            "          [ 3.4242e-03, -3.1589e-03, -9.5584e-03,  1.5445e-02],\n",
            "          [ 1.0861e-02, -1.7014e-03, -9.0190e-03, -1.1081e-02]],\n",
            "\n",
            "         [[-1.2031e-02, -1.4045e-02,  5.3277e-03, -9.3036e-03],\n",
            "          [ 9.5048e-03, -2.5895e-03,  1.3178e-02, -1.1654e-02],\n",
            "          [ 7.5400e-03, -1.3102e-02, -2.0790e-03,  6.1984e-03],\n",
            "          [-5.3312e-03, -7.7093e-03, -8.2713e-03, -1.1905e-02]],\n",
            "\n",
            "         [[-3.1757e-03,  7.0493e-03,  1.4451e-02,  2.4713e-04],\n",
            "          [-1.0157e-02,  6.6443e-03,  3.1881e-03, -4.7950e-03],\n",
            "          [ 6.6164e-03, -5.8694e-03, -1.4344e-02, -4.5718e-03],\n",
            "          [ 1.5384e-02,  4.4502e-03, -8.9141e-03,  7.1815e-03]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-3.8889e-03,  1.0350e-02,  5.1797e-04, -3.7267e-03],\n",
            "          [ 1.0198e-02, -8.6315e-03, -9.1781e-03,  1.5484e-02],\n",
            "          [ 3.1568e-03, -7.3649e-03,  9.1528e-03,  9.4950e-03],\n",
            "          [ 8.9310e-03, -8.1546e-03,  6.5693e-03, -5.2018e-04]],\n",
            "\n",
            "         [[ 1.2472e-02, -2.8670e-03, -1.3800e-02, -1.1478e-02],\n",
            "          [ 1.3317e-02,  9.8462e-04,  2.0640e-03, -1.3382e-03],\n",
            "          [-2.3249e-03,  3.0040e-03,  1.4073e-02, -8.2180e-03],\n",
            "          [ 1.4098e-02,  1.5268e-02,  1.1741e-02,  1.4436e-03]],\n",
            "\n",
            "         [[-1.4467e-02, -1.3683e-02,  5.4032e-03, -1.1681e-02],\n",
            "          [-5.1934e-03, -4.6392e-04,  1.8790e-03,  3.1951e-03],\n",
            "          [ 9.9157e-03, -1.3496e-02, -9.4258e-03, -1.0975e-04],\n",
            "          [ 1.3523e-02,  1.2674e-02,  1.0339e-02, -8.6600e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.3986e-02, -4.0652e-03,  5.9091e-04,  1.4674e-02],\n",
            "          [-1.5543e-02,  1.3898e-02, -1.0232e-02,  1.2460e-02],\n",
            "          [-1.3013e-02, -1.3795e-02,  1.1940e-02, -1.0402e-02],\n",
            "          [ 1.3751e-02,  4.9213e-03, -1.2455e-03, -1.1420e-02]],\n",
            "\n",
            "         [[-8.8208e-03, -7.0808e-03, -6.0061e-03, -1.4693e-03],\n",
            "          [-3.0421e-03, -1.4664e-02,  6.3125e-03, -1.2736e-02],\n",
            "          [ 7.5390e-03, -3.0848e-03,  9.6986e-03, -4.6318e-03],\n",
            "          [ 1.3104e-02,  1.3449e-02,  1.4807e-02,  9.5210e-03]],\n",
            "\n",
            "         [[-1.1196e-02, -1.2888e-02, -1.0266e-02, -4.3649e-03],\n",
            "          [ 7.3579e-03,  3.6517e-04,  1.0645e-02,  2.4374e-03],\n",
            "          [ 1.7498e-03, -1.1511e-03, -2.5861e-03,  6.2886e-03],\n",
            "          [-6.7587e-03, -1.5064e-02,  5.2752e-03,  4.1888e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 5.7392e-03, -4.5463e-03, -6.7578e-03,  3.2603e-03],\n",
            "          [ 1.3050e-02, -6.7814e-03,  7.7850e-03,  1.4605e-02],\n",
            "          [ 4.1861e-04, -9.0274e-03, -1.5507e-02,  4.1837e-04],\n",
            "          [-9.0079e-03,  2.0999e-03, -3.2513e-03,  1.3871e-02]],\n",
            "\n",
            "         [[-1.3026e-02, -1.0072e-02,  1.3172e-02,  1.2276e-02],\n",
            "          [ 1.5268e-03,  4.4717e-03, -9.5519e-03, -1.0406e-02],\n",
            "          [-4.4869e-03, -9.6009e-04,  3.7812e-03, -5.2292e-03],\n",
            "          [-9.7982e-03, -8.4976e-03,  9.0687e-03, -9.9157e-03]],\n",
            "\n",
            "         [[ 3.1202e-03, -1.1429e-02,  8.9647e-04,  1.0616e-02],\n",
            "          [-1.0391e-02,  6.6169e-03, -1.3685e-02,  1.0678e-02],\n",
            "          [ 3.1776e-03,  2.3864e-03, -6.8472e-03, -1.2321e-02],\n",
            "          [ 6.7922e-03,  4.7649e-04, -3.9593e-03,  1.3809e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-4.5225e-03,  8.7881e-03, -3.7466e-03, -1.4121e-02],\n",
            "          [ 4.2907e-03, -6.6031e-03, -1.6608e-03,  9.2936e-03],\n",
            "          [ 3.7604e-03,  8.9386e-03, -1.4292e-02,  1.1348e-02],\n",
            "          [-1.1373e-02,  1.3878e-02, -9.3135e-03,  6.4254e-03]],\n",
            "\n",
            "         [[ 8.4936e-03,  1.3975e-02,  1.0730e-02, -1.6703e-03],\n",
            "          [-3.1602e-03, -1.2702e-02,  5.8641e-03,  7.9656e-04],\n",
            "          [ 1.4855e-03,  1.5613e-02,  5.3007e-03,  4.1624e-03],\n",
            "          [-1.0247e-02,  1.3757e-02,  6.7961e-03, -7.7090e-03]],\n",
            "\n",
            "         [[ 9.5963e-03,  9.7335e-03,  6.2375e-03,  1.5286e-02],\n",
            "          [ 4.9300e-03,  1.1678e-03, -7.2341e-04, -3.5612e-03],\n",
            "          [-1.0367e-02, -7.5107e-04, -9.5489e-04,  1.4873e-02],\n",
            "          [ 1.3496e-02, -1.4357e-02, -1.4352e-02, -9.2606e-03]]],\n",
            "\n",
            "\n",
            "        [[[-8.9436e-03,  5.7203e-03,  1.4597e-02,  1.4242e-02],\n",
            "          [-1.3160e-02,  1.5698e-03,  9.4384e-03, -4.0703e-03],\n",
            "          [ 3.9836e-03, -3.8686e-03,  1.3820e-02, -1.5155e-02],\n",
            "          [ 1.0829e-02, -4.7205e-05, -1.3005e-02, -6.4869e-03]],\n",
            "\n",
            "         [[-1.4617e-02,  2.0821e-04, -1.0636e-02,  3.1908e-03],\n",
            "          [ 1.5218e-02, -1.2353e-02, -7.7838e-03,  7.1386e-03],\n",
            "          [-1.0476e-02, -1.4968e-02,  2.1877e-03, -1.3491e-05],\n",
            "          [ 1.0469e-02, -2.8222e-03,  6.3723e-04, -3.9872e-03]],\n",
            "\n",
            "         [[-4.3152e-03, -5.4506e-03,  1.3060e-02,  9.1490e-04],\n",
            "          [ 7.4727e-03,  5.3716e-03, -6.0151e-03,  1.3440e-02],\n",
            "          [-2.2900e-04,  6.6482e-03, -4.1525e-03,  1.1026e-02],\n",
            "          [-1.1640e-02,  1.1619e-02,  9.0868e-03,  2.1853e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.3677e-02, -8.7090e-03,  5.6610e-03,  1.4817e-02],\n",
            "          [-1.5411e-02,  3.4342e-03, -1.4210e-02,  3.0894e-03],\n",
            "          [ 1.3665e-02,  6.4990e-03, -1.1552e-02,  6.3908e-03],\n",
            "          [-2.1805e-03, -2.3767e-03,  3.7482e-03, -1.3556e-02]],\n",
            "\n",
            "         [[ 1.2782e-02, -1.3956e-02, -6.9263e-04,  1.4806e-02],\n",
            "          [ 4.8190e-03,  2.2653e-03, -1.0916e-02, -1.0298e-02],\n",
            "          [ 1.3214e-02, -7.6227e-03,  1.3929e-02,  3.0963e-03],\n",
            "          [-1.1662e-02,  2.4464e-05,  3.5869e-03,  1.5183e-02]],\n",
            "\n",
            "         [[-5.2110e-03,  2.0776e-04, -6.7123e-03,  7.7822e-03],\n",
            "          [ 8.2547e-03,  1.0737e-02,  6.9724e-03, -4.1592e-03],\n",
            "          [ 1.2974e-02,  7.2715e-03, -7.1132e-03, -5.8631e-03],\n",
            "          [ 6.6473e-03,  5.1526e-03, -1.3516e-02,  1.0433e-02]]]],\n",
            "       requires_grad=True)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[-0.6510, -0.4714, -0.5434, -1.3075],\n",
              "          [-0.5870, -0.0991, -0.6070, -0.2378],\n",
              "          [ 0.4861, -0.9595,  0.2480, -0.3261],\n",
              "          [-0.0642, -0.7183, -0.2637, -0.2647]],\n",
              "\n",
              "         [[-0.1186,  0.1650,  0.3444,  0.2002],\n",
              "          [-0.0928,  1.1424,  0.5338,  1.0565],\n",
              "          [ 0.5162,  0.7448,  0.0555,  0.7433],\n",
              "          [-0.6377, -0.6299,  0.0488, -0.0704]],\n",
              "\n",
              "         [[-0.8737, -0.1320, -0.6109, -0.8296],\n",
              "          [-0.3648,  0.1644,  0.6164, -0.0316],\n",
              "          [-0.4377,  0.4606, -0.0353, -0.1726],\n",
              "          [ 0.7799,  0.3783, -0.6962,  0.1628]]]], device='cuda:0',\n",
              "       grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 573
        }
      ]
    }
  ]
}