{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "PyTorch_StyleGAN2_Web(ONNX).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFjVwUap4Q0I",
        "outputId": "04990194-4a14-4471-df7d-0393cc35573f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "32JtLa3wJGNY",
        "outputId": "a11d7a55-ea5d-411d-8219-52a659614a02"
      },
      "source": [
        "'''%データセットのダウンロード\n",
        "!pip install gdown\n",
        "\n",
        "import gdown\n",
        "file_id = \"1-EyM2kIj24P6DtT-swZP8DLyBAPU1PkU\"\n",
        "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "output = \"dataset.zip\"\n",
        "gdown.download(url, output, quiet=False)\n",
        "!unzip \"dataset.zip\"\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'%データセットのダウンロード\\n!pip install gdown\\n\\nimport gdown\\nfile_id = \"1-EyM2kIj24P6DtT-swZP8DLyBAPU1PkU\"\\nurl = f\"https://drive.google.com/uc?id={file_id}\"\\noutput = \"dataset.zip\"\\ngdown.download(url, output, quiet=False)\\n!unzip \"dataset.zip\"\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8o_3yBZqJwOc",
        "outputId": "fa28442c-85a2-45bf-f3a1-46d85c8a536c"
      },
      "source": [
        "#import libraries\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda, Compose\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "print(torch.__version__)\n",
        "torch.manual_seed(0)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.8.1+cu101\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f7b0965bb30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lO_Cl9QBJyU0",
        "outputId": "87b68ed7-f841-47a2-b6a3-37ac667384bc"
      },
      "source": [
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using {} device\".format(device))\n",
        "LReLU_alpha = 0.2\n",
        "mapping_lamda = 0.01\n",
        "\n",
        "#https://github.com/yuuho/stylegans-pytorch/blob/master/network/stylegan2.py　を参考に\n",
        "from torch.nn import functional as F\n",
        "class modulated_conv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros'):\n",
        "        super(modulated_conv2d, self).__init__()\n",
        "        self.padding, self.stride = padding, stride\n",
        "\n",
        "        #self.weight = nn.Parameter(torch.randn(out_channels, in_channels, kernel_size, kernel_size))\n",
        "        #torch.nn.init.normal_(self.weight.data, mean=0.0, std=1.0)\n",
        "        self.weight = nn.Parameter(torch.zeros(out_channels, in_channels, kernel_size, kernel_size))\n",
        "        self.bias = nn.Parameter(torch.zeros(1,out_channels,1,1))\n",
        "        self.weight_scaler = 1 / (in_channels * kernel_size*kernel_size)**0.5\n",
        "\n",
        "    def forward(self,x,style,shape = None,web = True):\n",
        "        oC, iC, kH, kW = self.weight.shape\n",
        "        if shape is None:\n",
        "            N, iC, H, W = x.shape\n",
        "        else:\n",
        "            N, iC, H, W = shape\n",
        "\n",
        "        if web:\n",
        "            modulated_weight = self.weight.view(1,oC,iC,kH,kW) * self.weight_scaler \n",
        "            #modulated_weight = modulated_weight.repeat(N,1,1,1,1)\n",
        "            x = x * style.view(N,iC,1,1)\n",
        "            x = F.conv2d(x.view(1,N*iC,H,W), modulated_weight.view(N*oC,iC,kH,kW),\n",
        "                    padding=self.padding, stride=self.stride, groups=N).view(N,oC,H,W)\n",
        "            modulated_weight = modulated_weight * style.view(N,1,iC,1,1)\n",
        "            demod_norm = 1 / torch.sqrt((modulated_weight * modulated_weight).sum([2,3,4])  + 1e-8)\n",
        "            out = x * demod_norm.view(N, oC, 1, 1) + self.bias   \n",
        "\n",
        "        else:\n",
        "            modulated_weight = self.weight_scaler*self.weight.view(1,oC,iC,kH,kW) \\\n",
        "                                    * style.view(N,1,iC,1,1) \n",
        "\n",
        "            demod_norm = 1 / torch.sqrt((modulated_weight * modulated_weight).sum([2,3,4]) + 1e-8) # (N, oC)\n",
        "            demodulated_weight = modulated_weight * demod_norm.view(N, oC, 1, 1, 1) \n",
        "            #demodulated_weight = modulated_weight\n",
        "            out = F.conv2d(x.view(1,N*iC,H,W), demodulated_weight.view(N*oC,iC,kH,kW),\n",
        "                    padding=self.padding, stride=self.stride, groups=N).view(N,oC,H,W) + self.bias\n",
        "            \n",
        "        return out\n",
        "\n",
        "def alternative_Upsample(image,input_size):\n",
        "    batches, channels, h, w = input_size\n",
        "\n",
        "    x = image.view(batches, channels, h * w, 1)\n",
        "    x = torch.cat((x,x),3)\n",
        "    x = x.view(batches, channels, h, w * 2)\n",
        "    x = torch.cat((x,x),3)\n",
        "    x = x.view(batches, channels, h * 2, w * 2)\n",
        "    return x\n",
        "# Define model\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        dimensions = [256,256,256,256,128,128,128,64,64,64,32,32]\n",
        "        self.learning_const = torch.randn((1,dimensions[1],4,4),requires_grad=True, device = device)\n",
        "        self.mapping_network = self.generate_mapping_network(dimensions[0])\n",
        "\n",
        "        self.affine_4_0 = self.make_latent_to_style(dimensions[0],dimensions[1])\n",
        "        self.block_4_0 = self.generate_block(4,dimensions[1],dimensions[2])\n",
        "        self.noise_4_0 = torch.zeros((1,),requires_grad=True, device = device)\n",
        "        self.affine_4_1 = self.make_latent_to_style(dimensions[0],dimensions[2])\n",
        "        self.block_4_1 = self.generate_block(4,dimensions[2],dimensions[3])\n",
        "        self.noise_4_1 = torch.zeros((1,),requires_grad=True, device = device)\n",
        "        self.to_rgb_4 = nn.Conv2d(dimensions[3],3,kernel_size=1,stride=1,padding=0)\n",
        "\n",
        "        self.affine_8_0 = self.make_latent_to_style(dimensions[0],dimensions[3])\n",
        "        self.block_8_0 = self.generate_block(8,dimensions[3],dimensions[4],True)\n",
        "        self.noise_8_0 = torch.zeros((1,),requires_grad=True, device = device)\n",
        "        self.affine_8_1 = self.make_latent_to_style(dimensions[0],dimensions[4])\n",
        "        self.block_8_1 = self.generate_block(8,dimensions[4],dimensions[5])\n",
        "        self.noise_8_1 = torch.zeros((1,),requires_grad=True, device = device)\n",
        "        self.to_rgb_8 = nn.Conv2d(dimensions[5],3,kernel_size=1,stride=1,padding=0)\n",
        "\n",
        "        self.affine_16_0 = self.make_latent_to_style(dimensions[0],dimensions[5])\n",
        "        self.block_16_0 = self.generate_block(16,dimensions[5],dimensions[6],True)\n",
        "        self.noise_16_0 = torch.zeros((1,),requires_grad=True, device = device)\n",
        "        self.affine_16_1 = self.make_latent_to_style(dimensions[0],dimensions[6])\n",
        "        self.block_16_1 = self.generate_block(16,dimensions[6],dimensions[7])\n",
        "        self.noise_16_1 = torch.zeros((1,),requires_grad=True, device = device)\n",
        "        self.to_rgb_16 = nn.Conv2d(dimensions[7],3,kernel_size=1,stride=1,padding=0)\n",
        "\n",
        "        self.affine_32_0 = self.make_latent_to_style(dimensions[0],dimensions[7])\n",
        "        self.block_32_0 = self.generate_block(32,dimensions[7],dimensions[8],True)\n",
        "        self.noise_32_0 = torch.zeros((1,),requires_grad=True, device = device)\n",
        "        self.affine_32_1 = self.make_latent_to_style(dimensions[0],dimensions[8])\n",
        "        self.block_32_1 = self.generate_block(32,dimensions[8],dimensions[9])\n",
        "        self.noise_32_1 = torch.zeros((1,),requires_grad=True, device = device)\n",
        "        self.to_rgb_32 = nn.Conv2d(dimensions[9],3,kernel_size=1,stride=1,padding=0)\n",
        "\n",
        "        self.affine_64_0 = self.make_latent_to_style(dimensions[0],dimensions[9])\n",
        "        self.block_64_0 = self.generate_block(64,dimensions[9],dimensions[10],True)\n",
        "        self.noise_64_0 = torch.zeros((1,),requires_grad=True, device = device)\n",
        "        self.affine_64_1 = self.make_latent_to_style(dimensions[0],dimensions[10])\n",
        "        self.block_64_1 = self.generate_block(64,dimensions[10],dimensions[11])\n",
        "        self.noise_64_1 = torch.zeros((1,),requires_grad=True, device = device)\n",
        "        self.to_rgb_64 = nn.Conv2d(dimensions[11],3,kernel_size=1,stride=1,padding=0)\n",
        "        for i in range(5):\n",
        "            nn.init.normal_(getattr(self, 'to_rgb_{0}'.format(2 ** (i + 2))).weight, 0.0, 1.0 / getattr(self, 'to_rgb_{0}'.format(2 ** (i + 2))).weight.shape[1] ** 0.5)\n",
        "            nn.init.constant_(getattr(self, 'to_rgb_{0}'.format(2 ** (i + 2))).bias, 0.0)\n",
        "            \n",
        "            for j in range(2):\n",
        "                nn.init.normal_(getattr(self, 'affine_{0}_{1}'.format(2 ** (i + 2),j)).weight, 0.0, 1.0)\n",
        "                nn.init.constant_(getattr(self, 'affine_{0}_{1}'.format(2 ** (i + 2),j)).bias, 1.0)         \n",
        "\n",
        "    def generate_mapping_network(self,dimension = 512,number_of_layer = 1):\n",
        "       mapping_network = nn.Sequential()\n",
        "       for i in range(number_of_layer):\n",
        "           mapping_network.add_module('mapping_fc{0}'.format(i), nn.Linear(dimension,dimension))\n",
        "           mapping_network.add_module('mapping_lrelu{0}'.format(i), nn.LeakyReLU())\n",
        "           nn.init.normal_(getattr(mapping_network, 'mapping_fc{0}'.format(i)).weight, 0.0, 1.0)\n",
        "           nn.init.constant_(getattr(mapping_network, 'mapping_fc{0}'.format(i)).bias, 0)\n",
        "       return mapping_network\n",
        "\n",
        "    def generate_block(self,image_size = 4,in_dimension = 512, out_dimension = 512,upscaling = False):\n",
        "        return modulated_conv2d(in_dimension,out_dimension,kernel_size=3,stride=1,padding=1)\n",
        "        #return torch.nn.Conv2d(in_dimension,out_dimension,kernel_size=3,stride=1,padding=1)\n",
        "    def make_latent_to_style(self,latent_dimension = 512, target_dimension = 512):\n",
        "\n",
        "        return nn.Linear(latent_dimension, target_dimension)\n",
        "    def forward(self, z, stage = 1 ,alpha = 0, batches = 1,web = False):\n",
        "        style = self.mapping_network(z) * mapping_lamda\n",
        "        x = self.learning_const.repeat(int(batches),1,1,1)\n",
        "        for i in range(stage):\n",
        "            if i != 0:\n",
        "                if web:\n",
        "                    x = alternative_Upsample(x,(1,-1,2 ** (i + 1),2 ** (i + 1)))\n",
        "                else:\n",
        "                    x = torch.nn.functional.interpolate(x,scale_factor=2, mode='nearest')\n",
        "            for j in range(2):\n",
        "                if not (i == 0 and j == 0):\n",
        "                    affined_style = getattr(self, 'affine_{0}_{1}'.format(2 ** (i + 2),j))(style)\n",
        "                    x = getattr(self, 'block_{0}_{1}'.format(2 ** (i + 2),j))(x,affined_style,shape = (1,-1,2 ** (i + 2),2 ** (i + 2)))\n",
        "                    #return x\n",
        "                    x = F.leaky_relu(x,LReLU_alpha)\n",
        "                    #x = x + torch.randn(x.shape, device = device) *  getattr(self, 'noise_{0}_{1}'.format(2 ** (i + 2),j))\n",
        "\n",
        "        x_out = getattr(self, 'to_rgb_{0}'.format(2 ** (stage + 1)))(x)\n",
        "\n",
        "        if alpha != 0:\n",
        "            if web:\n",
        "                x = alternative_Upsample(x,(1,-1,2 ** (stage + 1),2 ** (stage + 1)))\n",
        "            else:\n",
        "                x = torch.nn.functional.interpolate(x,scale_factor=2, mode='nearest')\n",
        "            for j in range(2):\n",
        "                affined_style = getattr(self, 'affine_{0}_{1}'.format(2 ** (stage + 2),j))(style)\n",
        "                x = getattr(self, 'block_{0}_{1}'.format(2 ** (stage + 2),j))(x,affined_style,shape = (1,-1,2 ** (stage + 1),2 ** (stage + 1)))\n",
        "                x = F.leaky_relu(x,LReLU_alpha)\n",
        "                #x = x + torch.randn(x.shape, device = device) * getattr(self, 'noise_{0}_{1}'.format(2 ** (i + 2),j))\n",
        "            x = getattr(self, 'to_rgb_{0}'.format(2 ** (stage + 2)))(x)\n",
        "            if web:\n",
        "                x_out = alpha * x + alternative_Upsample(x_out,(1,-1,2 ** (stage + 1),2 ** (stage + 1))) * (1 - alpha)\n",
        "            else:\n",
        "                x_out = alpha * x + torch.nn.functional.interpolate(x,scale_factor=2, mode='nearest') * (1 - alpha)\n",
        "            \n",
        "        return x_out"
      ],
      "execution_count": 460,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dl4fVrUadKk4"
      },
      "source": [
        "# Define model\n",
        "from torch.autograd import Variable\n",
        "from torch import autograd\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        dimensions = [256,256,256,128,128,128,64,64,64,32,32,32]\n",
        "\n",
        "        self.from_rgb_4 = nn.Conv2d(3,dimensions[2],kernel_size=1,stride=1,padding=0)\n",
        "        self.block_4_0 = self.generate_block(4,dimensions[2],dimensions[1])\n",
        "        self.block_4_1 = self.generate_block(4,dimensions[1],dimensions[0])\n",
        "\n",
        "        self.from_rgb_8 = nn.Conv2d(3,dimensions[4],kernel_size=1,stride=1,padding=0)\n",
        "        self.block_8_0 = self.generate_block(8,dimensions[4],dimensions[3])\n",
        "        self.block_8_1 = self.generate_block(8,dimensions[3],dimensions[2])\n",
        "\n",
        "        self.from_rgb_16 = nn.Conv2d(3,dimensions[6],kernel_size=1,stride=1,padding=0)\n",
        "        self.block_16_0 = self.generate_block(16,dimensions[6],dimensions[5])\n",
        "        self.block_16_1 = self.generate_block(16,dimensions[5],dimensions[4])\n",
        "\n",
        "        self.from_rgb_32 = nn.Conv2d(3,dimensions[8],kernel_size=1,stride=1,padding=0)\n",
        "        self.block_32_0 = self.generate_block(32,dimensions[8],dimensions[7])\n",
        "        self.block_32_1 = self.generate_block(32,dimensions[7],dimensions[6])\n",
        "\n",
        "        self.from_rgb_64 = nn.Conv2d(3,dimensions[10],kernel_size=1,stride=1,padding=0)\n",
        "        self.block_64_0 = self.generate_block(64,dimensions[10],dimensions[9])\n",
        "        self.block_64_1 = self.generate_block(64,dimensions[9],dimensions[8])\n",
        "\n",
        "        self.final_conv = nn.Conv2d(dimensions[0],dimensions[0],kernel_size=4,stride=1,padding=0)\n",
        "        self.linear = nn.Linear(dimensions[0],1)\n",
        "        print(self.final_conv.weight)\n",
        "        nn.init.normal_(self.final_conv.weight, 0.0, 1.0 / (4*4**dimensions[0]) ** 0.5)\n",
        "        nn.init.constant_(self.final_conv.bias, 0)\n",
        "        nn.init.normal_(self.linear.weight, 0.0, 1.0)\n",
        "        nn.init.constant_(self.linear.bias, 0)\n",
        "        for i in range(5):\n",
        "            nn.init.normal_(getattr(self, 'from_rgb_{0}'.format(2 ** (i + 2))).weight, 0.0, 1.0 / getattr(self, 'from_rgb_{0}'.format(2 ** (i + 2))).weight.shape[1] ** 0.5)\n",
        "            nn.init.constant_(getattr(self, 'from_rgb_{0}'.format(2 ** (i + 2))).bias, 0.0)\n",
        "\n",
        "    def generate_block(self,image_size = 4,in_dimension = 512, out_dimension = 512):\n",
        "        block = nn.Sequential()\n",
        "\n",
        "        block.add_module('Conv', nn.Conv2d(in_dimension,out_dimension,kernel_size=3,stride=1,padding=1))\n",
        "        block.add_module('relu', nn.LeakyReLU(LReLU_alpha))\n",
        "        nn.init.normal_(block.Conv.weight, 0.0, 1.0)\n",
        "        nn.init.constant_(block.Conv.bias, 0)\n",
        "\n",
        "        return block\n",
        "    #https://github.com/Zeleni9/pytorch-wgan/blob/master/models/wgan_gradient_penalty.py を改変\n",
        "    def calculate_gradient_penalty(self, real_images, fake_images,batch_size,stage,alpha):\n",
        "        eta = torch.FloatTensor(batch_size,1,1,1).uniform_(0,1).to(device)\n",
        "        eta = eta.expand(batch_size, real_images.size(1), real_images.size(2), real_images.size(3))\n",
        "\n",
        "        interpolated = eta * real_images + ((1 - eta) * fake_images)\n",
        "\n",
        "        # define it to calculate gradient\n",
        "        interpolated = Variable(interpolated, requires_grad=True)\n",
        "        # calculate probability of interpolated examples\n",
        "        prob_interpolated = self(interpolated,stage,alpha)\n",
        "        # calculate gradients of probabilities with respect to examples\n",
        "        gradients = autograd.grad(outputs=prob_interpolated, inputs=interpolated,\n",
        "                              grad_outputs=torch.ones(\n",
        "                                  prob_interpolated.size()).to(device),\n",
        "                              create_graph=True, retain_graph=True)[0]\n",
        "        grad_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
        "        return grad_penalty\n",
        "    \n",
        "    def forward(self, image, stage = 1 ,alpha = 0, batches = 1):\n",
        "        if alpha != 0:\n",
        "            x = getattr(self, 'from_rgb_{0}'.format(2 ** (stage + 2)))(image)\n",
        "            for j in range(2):\n",
        "                x = getattr(self, 'block_{0}_{1}'.format(2 ** (stage + 2),j))(x)\n",
        "            x = torch.nn.functional.interpolate(x,scale_factor=0.5, mode='nearest')\n",
        "            x2 = getattr(self, 'from_rgb_{0}'.format(2 ** (stage + 1)))(torch.nn.functional.interpolate(image,scale_factor=0.5, mode='nearest'))\n",
        "            x = x * alpha + x2 * (1 - alpha)\n",
        "        else:\n",
        "            x = getattr(self, 'from_rgb_{0}'.format(2 ** (stage + 1)))(image)\n",
        "\n",
        "        for i in range(stage):\n",
        "            for j in range(2):\n",
        "                if not (i == stage - 1 and j == 1):\n",
        "                    x = getattr(self, 'block_{0}_{1}'.format(2 ** (stage + 1 - i),j))(x)\n",
        "            if i != stage - 1:\n",
        "                x = torch.nn.functional.interpolate(x,scale_factor=0.5, mode='nearest')\n",
        "\n",
        "        x = self.final_conv(x)\n",
        "        \n",
        "        x = nn.LeakyReLU(LReLU_alpha)(x)\n",
        "        x = nn.Flatten()(x)\n",
        "        return  self.linear(x)"
      ],
      "execution_count": 461,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evars0QQOPuM"
      },
      "source": [
        "class trainer():\n",
        "    def __init__(self,learning_rate = 0.0002):\n",
        "        self.alpha = 0\n",
        "        self.stage = 0\n",
        "        self.MAX_STAGE = 4\n",
        "        self.BATCH_SIZE = 32\n",
        "        self.n_critic = 1\n",
        "        self.gp_lamda = 1\n",
        "        self.g = Generator().to(device)\n",
        "        self.d = Discriminator().to(device)\n",
        "        self.G_optimizer = torch.optim.Adam(self.g.parameters(), lr=learning_rate)\n",
        "        self.D_optimizer = torch.optim.Adam(self.d.parameters(), lr=learning_rate)\n",
        "        self.NUM_EPOCH_NO_ALPHA = 50\n",
        "        self.NUM_EPOCH_WITH_ALPHA = 50\n",
        "        self.z_dimension = 256\n",
        "    def train(self):\n",
        "        MAX_STAGE = self.MAX_STAGE\n",
        "        BATCH_SIZE = self.BATCH_SIZE\n",
        "        n_critic = self.n_critic\n",
        "        gp_lamda = self.gp_lamda\n",
        "        g = self.g\n",
        "        d = self.d\n",
        "        G_optimizer = self.G_optimizer\n",
        "        D_optimizer = self.D_optimizer\n",
        "        NUM_EPOCH_NO_ALPHA = self.NUM_EPOCH_NO_ALPHA\n",
        "        NUM_EPOCH_WITH_ALPHA = self.NUM_EPOCH_WITH_ALPHA\n",
        "        z_dimension = self.z_dimension\n",
        "        training_data = datasets.CIFAR10(\n",
        "            root=\"data\",\n",
        "            train=True,\n",
        "            download=True,\n",
        "            transform=ToTensor(),\n",
        "        )\n",
        "        train_dataloader = DataLoader(training_data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "        while True:\n",
        "            for epoch in range(NUM_EPOCH_NO_ALPHA + NUM_EPOCH_WITH_ALPHA):\n",
        "                number_of_batch = 0\n",
        "                for X, _ in train_dataloader:\n",
        "                    X = X * 2 - 1\n",
        "                    \n",
        "                    alpha = 0\n",
        "                    if int(self.stage) % 2 == 1:\n",
        "                        self.stage += 1 / (len(train_dataloader) * NUM_EPOCH_WITH_ALPHA) \n",
        "                    elif math.ceil(self.stage / 2) < self.MAX_STAGE:\n",
        "                        self.stage += 1 / (len(train_dataloader) * NUM_EPOCH_NO_ALPHA)\n",
        "                    stage = math.ceil(self.stage / 2)    \n",
        "                    if int(self.stage) % 2 == 1:\n",
        "                        alpha = self.stage - int(self.stage)\n",
        "                    \n",
        "                    for _ in range(n_critic):\n",
        "                        z = torch.randn((BATCH_SIZE,z_dimension),device = device)\n",
        "                        generated = g(z, stage = stage ,alpha = alpha, batches = BATCH_SIZE)\n",
        "                        X = X.to(device)\n",
        "                        X = torch.nn.functional.interpolate(X,size=generated.size(2), mode='Nearest')\n",
        "                        y_real = d(X, stage = stage ,alpha = alpha, batches = BATCH_SIZE)\n",
        "                        y_fake = d(generated, stage = stage ,alpha = alpha, batches = BATCH_SIZE)\n",
        "                        d_loss = torch.mean(y_fake) - torch.mean(y_real) + d.calculate_gradient_penalty(X,generated,BATCH_SIZE,stage,alpha) * gp_lamda\n",
        "                        Wasserstein_loss =  (torch.mean(y_fake) - torch.mean(y_real)).data\n",
        "\n",
        "                        D_optimizer.zero_grad()\n",
        "                        d_loss.backward()\n",
        "                        D_optimizer.step()\n",
        "                    g_loss = 'undefined'\n",
        "                    if Wasserstein_loss <= 0:\n",
        "                        z = torch.randn((BATCH_SIZE,z_dimension),device = device)\n",
        "                        generated = g(z, stage = stage ,alpha = alpha, batches = BATCH_SIZE)\n",
        "                        y_fake = d(generated, stage = stage ,alpha = alpha, batches = BATCH_SIZE)\n",
        "                        g_loss = -torch.mean(y_fake)\n",
        "                        G_optimizer.zero_grad()\n",
        "                        g_loss.backward()\n",
        "                        G_optimizer.step()\n",
        "\n",
        "                    \n",
        "\n",
        "                    if number_of_batch % 100 == 0:\n",
        "                        print('epoch:{}, batch:{},stage:{},alpha{},g_loss:{}, d_loss:{}, Wasserstein Loss:{}'.format(epoch,number_of_batch,stage,alpha,g_loss,d_loss,Wasserstein_loss))\n",
        "                        show_image(generated)\n",
        "                        show_image(X)\n",
        "                        torch.save(self, '/content/drive/MyDrive/StyleGAN2/model.pth')\n",
        "                    number_of_batch += 1"
      ],
      "execution_count": 462,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMy29H2BzU5w",
        "outputId": "b023b517-3424-47ca-ed65-5119996bfc05"
      },
      "source": [
        "try:\n",
        "    train = torch.load('/content/drive/MyDrive/StyleGAN2/model.pth')\n",
        "except:\n",
        "    train = trainer()\n",
        "    print('load failed')\n",
        "\n",
        "dummy_input = torch.randn(1, train.z_dimension, device=device)\n",
        "train.g.learning_const.requires_grad = False\n",
        "for i in range(5):  \n",
        "    for j in range(2):\n",
        "        getattr(train.g, 'noise_{0}_{1}'.format(2 ** (i + 2),j)).requires_grad = False\n",
        "        getattr(train.g, 'noise_{0}_{1}'.format(2 ** (i + 2),j)).requires_grad = False\n",
        "stage = torch.tensor(2, dtype=torch.int)\n",
        "alpha = torch.tensor(0, dtype=torch.int)\n",
        "batches = torch.tensor(1, dtype=torch.int)\n",
        "web = torch.tensor(True, dtype=torch.bool)\n",
        "torch.onnx.export(train.g, (dummy_input,stage,alpha,batches,web), 'generator.onnx', opset_version= 9)"
      ],
      "execution_count": 466,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:129: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:132: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:146: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBqbMLLl5Ip8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "f173be4a-a892-4d70-9fbd-a409975c4775"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('generator.onnx')"
      ],
      "execution_count": 467,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_d523560f-b1c1-40f8-8a61-615fb97e5511\", \"generator.onnx\", 11048465)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMqPXkseJ6xc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f42475a9-6cde-4991-b203-7bc14dbdd350"
      },
      "source": [
        "#train = torch.load('/content/drive/MyDrive/StyleGAN2/model.pth')\n",
        "train = trainer()\n",
        "z = torch.zeros(1, train.z_dimension, device=device)\n",
        "train.g(z,web=True)"
      ],
      "execution_count": 465,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[[[-7.9786e-03,  1.9255e-03,  6.2801e-03, -3.8985e-03],\n",
            "          [ 1.2001e-04, -6.2561e-03, -1.0863e-02,  9.9125e-03],\n",
            "          [ 1.2752e-02,  7.3303e-03,  1.0446e-02,  4.7783e-03],\n",
            "          [ 1.4986e-02, -1.3861e-03, -1.3183e-02,  3.6752e-03]],\n",
            "\n",
            "         [[ 1.1659e-02,  1.4366e-02,  8.2056e-03,  1.0706e-02],\n",
            "          [ 8.3887e-04, -2.2423e-03, -1.5331e-02,  4.6258e-03],\n",
            "          [ 1.0280e-03,  1.4662e-02, -5.6100e-03, -8.2037e-03],\n",
            "          [-7.4028e-03,  3.7505e-03, -6.0086e-03, -1.1732e-03]],\n",
            "\n",
            "         [[ 2.7164e-03,  1.4822e-02, -7.0147e-03, -1.3325e-02],\n",
            "          [-1.3047e-02,  1.3750e-02, -9.1688e-03,  8.5654e-03],\n",
            "          [ 1.0336e-02,  4.0142e-03,  7.6700e-03, -5.4893e-04],\n",
            "          [-7.2998e-03,  1.2569e-03, -1.3856e-02, -9.8828e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.4680e-02, -4.3292e-03, -1.8872e-03,  9.4592e-03],\n",
            "          [ 1.6328e-05,  2.8041e-03, -1.5036e-02, -7.4368e-03],\n",
            "          [ 1.4915e-02,  4.0846e-03, -1.0791e-02, -5.9849e-03],\n",
            "          [ 9.8127e-03, -6.1038e-03,  6.5387e-03,  5.8059e-03]],\n",
            "\n",
            "         [[-1.4769e-02,  1.4523e-02, -1.9146e-03,  4.8333e-03],\n",
            "          [ 1.4007e-03,  8.6750e-03, -1.1862e-02,  5.1572e-03],\n",
            "          [ 1.1922e-02, -1.8811e-03, -1.3342e-02,  7.6579e-03],\n",
            "          [ 3.7258e-03,  1.0084e-02, -8.1803e-03,  2.7786e-03]],\n",
            "\n",
            "         [[-1.4368e-02, -4.7220e-03, -3.7621e-03, -4.5056e-03],\n",
            "          [-1.4689e-02, -2.4319e-03,  3.2955e-03, -2.4059e-03],\n",
            "          [ 8.6739e-03, -5.8143e-03,  3.2231e-04, -2.0277e-03],\n",
            "          [ 1.0953e-02,  1.0285e-03,  1.6272e-03,  1.5536e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.0777e-02, -1.0743e-03,  1.1741e-02,  4.4202e-03],\n",
            "          [-1.2979e-02, -2.6206e-03,  8.5372e-03,  1.0615e-02],\n",
            "          [ 1.4543e-02, -5.6751e-04, -1.1373e-02,  5.8090e-03],\n",
            "          [-6.0463e-03, -8.5834e-03, -7.0960e-03,  1.3734e-02]],\n",
            "\n",
            "         [[ 5.1508e-03, -5.6768e-03,  1.4308e-02,  9.3378e-03],\n",
            "          [-1.1276e-02, -3.5356e-03, -9.5502e-04, -1.4863e-02],\n",
            "          [-8.2848e-03,  2.5530e-03,  1.4228e-02,  6.6519e-03],\n",
            "          [-1.4262e-02, -7.8074e-03,  1.1944e-02, -8.2667e-03]],\n",
            "\n",
            "         [[-8.3613e-03,  1.5560e-02,  1.3914e-02, -3.6124e-03],\n",
            "          [-8.9102e-04,  1.3483e-02,  5.2212e-03,  1.4874e-02],\n",
            "          [-4.8046e-03,  1.1418e-03, -2.1813e-03, -1.3972e-02],\n",
            "          [ 2.7306e-03, -7.1651e-03,  6.2112e-03,  7.1579e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 6.1111e-03,  8.2030e-03,  8.4468e-03,  1.3182e-02],\n",
            "          [ 7.9597e-03,  1.3043e-02, -5.5017e-03,  7.5466e-03],\n",
            "          [-6.3711e-03,  7.3676e-04, -5.4668e-03, -8.4154e-03],\n",
            "          [-5.1594e-03,  1.0739e-02,  8.3443e-03, -6.4517e-03]],\n",
            "\n",
            "         [[ 4.6791e-03, -1.5399e-02,  1.0040e-02,  1.1135e-02],\n",
            "          [-1.2141e-02, -5.8852e-03, -8.0195e-03, -1.0357e-02],\n",
            "          [ 1.1858e-02,  1.5501e-02,  3.0758e-03, -3.6228e-03],\n",
            "          [-5.0797e-03,  7.2927e-03, -1.0727e-02, -1.2838e-02]],\n",
            "\n",
            "         [[-5.8841e-03,  6.7329e-03, -1.4490e-02, -6.0893e-03],\n",
            "          [-1.0061e-02,  1.5835e-04,  1.4855e-02, -1.1416e-02],\n",
            "          [-1.2982e-02, -1.3172e-02,  1.0681e-02,  1.3254e-02],\n",
            "          [ 7.1654e-03,  3.3396e-03,  1.9484e-03,  6.3387e-03]]],\n",
            "\n",
            "\n",
            "        [[[-5.2554e-03,  2.9730e-03,  6.8423e-03, -4.4675e-03],\n",
            "          [ 9.6301e-03, -1.1330e-02, -1.9816e-03, -1.5202e-03],\n",
            "          [-1.5104e-02, -6.0316e-03,  7.6121e-03,  1.2439e-02],\n",
            "          [-1.1259e-02,  4.4124e-03, -1.3876e-02, -4.2028e-03]],\n",
            "\n",
            "         [[ 4.3318e-03,  3.4440e-03, -7.4847e-04, -4.8000e-03],\n",
            "          [ 1.1098e-02, -1.5344e-02,  1.3853e-02,  8.7432e-03],\n",
            "          [-3.2041e-04,  1.5207e-02,  5.2442e-03, -9.0473e-03],\n",
            "          [-1.0995e-02, -3.5167e-03, -1.3894e-02, -1.4166e-03]],\n",
            "\n",
            "         [[ 3.7823e-03, -7.3315e-03,  4.1932e-03, -6.7257e-03],\n",
            "          [ 1.4376e-03, -4.8007e-04, -1.1902e-02, -1.1691e-02],\n",
            "          [ 1.0351e-02, -1.8762e-03, -1.3885e-02,  5.1956e-03],\n",
            "          [-7.5921e-03,  1.5202e-02,  8.7907e-03,  4.3840e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.0515e-04, -1.3845e-02,  3.4703e-05, -2.5560e-03],\n",
            "          [-1.5291e-02, -1.0719e-02, -2.8795e-03, -1.5310e-02],\n",
            "          [ 9.3887e-03,  5.3537e-03,  2.5098e-03, -7.3683e-05],\n",
            "          [ 3.1822e-04, -1.0549e-02, -2.2357e-03, -1.4469e-02]],\n",
            "\n",
            "         [[-6.2974e-03, -1.5445e-02,  2.8858e-03,  5.9380e-04],\n",
            "          [-7.9554e-03, -1.2518e-02, -3.0178e-04,  3.2256e-03],\n",
            "          [-2.8978e-03, -1.0617e-03,  1.0032e-02,  7.0313e-03],\n",
            "          [ 3.6754e-03, -1.4731e-02,  1.3308e-02,  1.0180e-02]],\n",
            "\n",
            "         [[ 2.7968e-03,  1.0642e-02, -2.0312e-03, -1.9378e-03],\n",
            "          [ 7.8139e-03,  8.9700e-03,  6.0704e-03, -1.2267e-02],\n",
            "          [ 8.4286e-04, -1.2328e-02, -5.3228e-03,  5.3446e-03],\n",
            "          [-5.2242e-03,  7.8517e-03,  2.6174e-03, -6.0275e-03]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-1.4548e-02, -7.5601e-03, -1.1267e-02, -8.9885e-03],\n",
            "          [-1.3713e-02, -1.1049e-02, -6.4444e-03, -5.1320e-03],\n",
            "          [-1.2402e-02, -9.1857e-03,  1.0328e-02,  1.5679e-03],\n",
            "          [ 2.6120e-04,  1.2846e-02,  5.1524e-03,  1.4040e-02]],\n",
            "\n",
            "         [[-9.9073e-04,  4.4165e-04, -1.9266e-03,  3.6859e-04],\n",
            "          [-2.8704e-03, -8.3547e-04,  9.9145e-03,  8.1877e-03],\n",
            "          [-3.4716e-03, -1.3213e-02,  9.8628e-03,  1.4483e-02],\n",
            "          [ 1.3378e-02,  1.0993e-02, -1.9140e-03, -3.3199e-03]],\n",
            "\n",
            "         [[-8.5529e-03, -1.3334e-02, -1.4382e-02, -1.0347e-02],\n",
            "          [-4.1838e-03,  1.2877e-02, -1.3123e-02, -9.7624e-03],\n",
            "          [-7.1065e-03, -1.0058e-02,  5.7810e-03,  1.4598e-02],\n",
            "          [ 6.1995e-03, -5.0609e-03, -3.4386e-03,  1.4378e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-7.5678e-03,  6.2078e-03, -1.2250e-02, -2.4811e-03],\n",
            "          [-1.2337e-02,  8.2377e-03,  1.4240e-03, -1.0047e-02],\n",
            "          [-8.6605e-03,  7.1549e-03, -9.3203e-03,  1.5596e-02],\n",
            "          [-1.4867e-02,  8.8039e-03,  4.0497e-03,  8.1332e-03]],\n",
            "\n",
            "         [[-1.4492e-02, -1.4983e-02, -1.3687e-02,  1.9617e-03],\n",
            "          [-4.1891e-03,  1.4473e-02,  1.5553e-02, -3.7071e-03],\n",
            "          [-9.1361e-04, -1.5245e-02, -6.9379e-03, -7.9971e-03],\n",
            "          [ 1.3436e-03,  8.9209e-03,  1.0585e-03,  7.2811e-03]],\n",
            "\n",
            "         [[ 1.0054e-02,  7.0181e-03,  7.4744e-03, -4.4739e-03],\n",
            "          [-1.1865e-02, -1.2500e-03,  1.0084e-02, -4.8575e-03],\n",
            "          [-1.0778e-02, -1.3721e-02, -6.3942e-04,  6.4072e-03],\n",
            "          [ 1.0066e-02,  3.3378e-03, -1.2482e-02,  9.5866e-03]]],\n",
            "\n",
            "\n",
            "        [[[-7.4509e-03, -1.4502e-02,  9.9105e-03,  7.0484e-03],\n",
            "          [ 6.4741e-03, -1.5597e-03, -1.5438e-02, -9.2550e-04],\n",
            "          [ 1.3795e-02,  4.9677e-03, -2.3210e-03, -6.5150e-03],\n",
            "          [-7.4177e-03, -4.5934e-03,  8.8361e-03,  1.0524e-02]],\n",
            "\n",
            "         [[ 1.4581e-02, -2.5169e-03, -4.0799e-03, -3.1686e-03],\n",
            "          [ 1.3841e-02,  1.1280e-02, -1.4898e-02, -1.7618e-04],\n",
            "          [-1.0978e-02, -1.1281e-02,  1.5025e-02, -1.1481e-02],\n",
            "          [ 1.1783e-02,  1.4935e-03, -6.5284e-03, -5.6409e-03]],\n",
            "\n",
            "         [[-1.1022e-02, -6.9399e-03, -8.8692e-03, -1.4460e-03],\n",
            "          [ 5.8928e-03, -1.3166e-02,  4.8979e-03,  1.5519e-02],\n",
            "          [ 5.5217e-03, -9.3989e-03,  5.2079e-03, -2.7531e-04],\n",
            "          [ 1.3958e-02,  1.0628e-02, -4.7295e-03,  1.3568e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 6.8996e-03, -9.6744e-03,  9.1606e-03,  1.3135e-02],\n",
            "          [-8.8810e-03, -5.6310e-03, -3.5333e-03,  3.1462e-03],\n",
            "          [-7.3917e-04,  1.0134e-02,  1.4698e-02,  1.0500e-02],\n",
            "          [ 1.4567e-02, -1.4508e-02,  7.4806e-03, -5.4340e-03]],\n",
            "\n",
            "         [[-5.5661e-03, -1.0070e-02,  1.5517e-02, -7.4440e-03],\n",
            "          [ 6.3027e-03, -1.1601e-02,  5.4111e-03, -1.4597e-02],\n",
            "          [ 1.1625e-02,  7.3177e-04, -1.3284e-02, -9.4814e-03],\n",
            "          [-5.8082e-03,  8.5277e-03,  7.2191e-03,  9.2958e-03]],\n",
            "\n",
            "         [[ 1.2042e-02,  8.2493e-03,  6.9396e-03, -1.2080e-02],\n",
            "          [-1.5375e-03,  7.2648e-03,  7.1770e-03, -8.1002e-03],\n",
            "          [ 1.3948e-03, -1.9250e-03, -1.4776e-02,  1.5559e-02],\n",
            "          [ 1.9696e-04,  1.1181e-02,  3.3535e-03,  4.6748e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 5.8444e-03, -1.1371e-02, -1.1250e-02,  3.7975e-03],\n",
            "          [ 1.1399e-02,  5.9506e-03,  1.3352e-02, -6.6118e-03],\n",
            "          [ 1.3244e-02, -8.0711e-03, -2.0933e-03,  1.0655e-04],\n",
            "          [-1.3397e-02, -3.7145e-03,  8.2776e-03,  8.4558e-03]],\n",
            "\n",
            "         [[ 1.3962e-02, -1.0054e-03, -3.7300e-03,  9.4538e-03],\n",
            "          [ 1.0416e-02,  1.2715e-04,  7.0806e-03,  1.7452e-03],\n",
            "          [-1.0481e-02, -1.2563e-02, -6.8296e-03,  8.1438e-04],\n",
            "          [ 9.0923e-03, -1.2442e-03,  6.2234e-03,  1.3063e-02]],\n",
            "\n",
            "         [[ 4.4002e-03, -1.4595e-02, -7.3014e-03,  1.6303e-03],\n",
            "          [ 1.3564e-02,  2.2979e-03,  4.8244e-03,  7.3035e-03],\n",
            "          [ 9.7918e-03,  1.1178e-03,  5.5154e-03, -4.3302e-03],\n",
            "          [-1.2597e-02,  8.0464e-03, -1.4591e-02,  1.2176e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-8.7349e-03,  1.2191e-02,  8.3416e-04, -1.4124e-02],\n",
            "          [ 1.4155e-02,  6.0016e-03, -1.1667e-02, -2.6619e-03],\n",
            "          [ 2.3154e-03,  5.0613e-03,  1.1437e-02,  1.0409e-03],\n",
            "          [-1.4809e-02, -3.9179e-03,  1.4529e-02,  7.1677e-03]],\n",
            "\n",
            "         [[ 3.0138e-03, -1.0288e-02, -1.2348e-02,  1.4511e-02],\n",
            "          [-1.3697e-02,  1.0423e-02,  5.7943e-03, -1.2818e-02],\n",
            "          [-9.3089e-03,  3.6631e-03, -2.1645e-03, -1.0456e-02],\n",
            "          [-2.9603e-04, -2.1061e-03,  1.2603e-02, -8.5060e-03]],\n",
            "\n",
            "         [[ 6.2678e-03, -1.4895e-03,  1.3361e-02,  1.4398e-02],\n",
            "          [-1.5955e-04, -1.0612e-03,  1.4482e-02, -1.0924e-02],\n",
            "          [ 1.2308e-02,  3.7909e-03,  8.7369e-03, -2.9808e-03],\n",
            "          [-1.2561e-02,  3.5363e-03, -4.0388e-04,  1.7786e-03]]]],\n",
            "       requires_grad=True)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[0., 0., 0., 0.],\n",
              "          [0., 0., 0., 0.],\n",
              "          [0., 0., 0., 0.],\n",
              "          [0., 0., 0., 0.]],\n",
              "\n",
              "         [[0., 0., 0., 0.],\n",
              "          [0., 0., 0., 0.],\n",
              "          [0., 0., 0., 0.],\n",
              "          [0., 0., 0., 0.]],\n",
              "\n",
              "         [[0., 0., 0., 0.],\n",
              "          [0., 0., 0., 0.],\n",
              "          [0., 0., 0., 0.],\n",
              "          [0., 0., 0., 0.]]]], device='cuda:0', grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 465
        }
      ]
    }
  ]
}