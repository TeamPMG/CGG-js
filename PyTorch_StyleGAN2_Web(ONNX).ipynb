{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "PyTorch_StyleGAN2_Web(ONNX).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFjVwUap4Q0I",
        "outputId": "14e557c6-8fe7-46a9-c4d8-3101219f5094"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.activity.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fexperimentsandconfigs%20https%3a%2f%2fwww.googleapis.com%2fauth%2fphotos.native&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "4/1AX4XfWgvkyJ9JtWvRCduTRDOGqrLaZlim8xaeTW3HSKV3QXVTOeTLFI15T4\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32JtLa3wJGNY",
        "outputId": "a00bbadc-45d1-473b-9177-a9a493e0e346"
      },
      "source": [
        "#データセットのダウンロード\n",
        "!pip install gdown\n",
        "\n",
        "import gdown\n",
        "#file_id = \"1-EyM2kIj24P6DtT-swZP8DLyBAPU1PkU\" \n",
        "file_id = \"1l8z-u2VW25UGlE0fhzZrB_ZrTLvYs961\"\n",
        "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "output = \"dataset.zip\"\n",
        "gdown.download(url, output, quiet=False)\n",
        "!unzip -q \"dataset.zip\" "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (3.6.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (3.0.4)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1l8z-u2VW25UGlE0fhzZrB_ZrTLvYs961\n",
            "To: /content/dataset.zip\n",
            "903MB [00:15, 58.9MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8o_3yBZqJwOc"
      },
      "source": [
        "#import libraries\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.transforms import ToTensor, Lambda, Compose\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "print(torch.__version__)\n",
        "torch.manual_seed(0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlMwXtkvBkfg"
      },
      "source": [
        "#util for training\n",
        "from IPython.display import Image,display_png\n",
        "from PIL import Image\n",
        "import math\n",
        "\n",
        "def combine_images(generated_images):\n",
        "    total = generated_images.shape[0]\n",
        "    cols = int(math.sqrt(total))\n",
        "    rows = math.ceil(float(total)/cols)\n",
        "    width, height = generated_images.shape[1:3]\n",
        "    combined_image = np.zeros((width*cols, height*rows,3),\n",
        "                              dtype=generated_images.dtype)\n",
        "    #coreturn combined_image\n",
        "\n",
        "    for index, image in enumerate(generated_images):\n",
        "        i = index % cols\n",
        "        j = int(index/cols)\n",
        "        combined_image[width*i:width*(i+1), height*j:height*(j+1),0:3] = image[:,:,0:3]\n",
        "    return combined_image\n",
        "\n",
        "def show_image(result,name):\n",
        "    generated_image = result.to('cpu').detach().numpy().copy()\n",
        "    generated_image = generated_image * 127.5 + 127.5\n",
        "    generated_image = np.where(generated_image < 0, 0, generated_image)\n",
        "    generated_image = np.where(generated_image > 255, 255, generated_image)\n",
        "    generated_image=np.transpose(generated_image, (0, 2, 3, 1))\n",
        "    generated_image = combine_images(generated_image)\n",
        "    generated_image = generated_image.astype(np.uint8)\n",
        "    image = Image.fromarray(generated_image).save('/content/drive/MyDrive/StyleGAN2/generated/' + name + '.png')\n",
        "    #display_png(image)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmcayDWUVwI0"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.io import read_image\n",
        "import glob\n",
        "\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, img_dir, transforms):\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transforms\n",
        "        self.file_names = glob.glob(self.img_dir + \"*\")\n",
        "        self.len = len(self.file_names)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    def __getitem__(self, idx, size = 512):\n",
        "        img_path = self.file_names[idx]#os.path.join(self.img_dir, '../' + )\n",
        "        \n",
        "        #image = torch.nn.functional.interpolate((read_image(img_path).unsqueeze(0) - 127.5) / 127.5, size=(size,size),mode='bilinear').squeeze(0)\n",
        "\n",
        "        return self.transform(read_image(img_path)) / 255"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lO_Cl9QBJyU0",
        "outputId": "aab4a15e-b7fc-497d-d3a6-316bd9b5718a"
      },
      "source": [
        "\n",
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using {} device\".format(device))\n",
        "LReLU_alpha = 0.2\n",
        "\n",
        "#https://github.com/yuuho/stylegans-pytorch/blob/master/network/stylegan2.py　を参考に\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class equalized_linear(nn.Module):\n",
        "    def __init__(self,in_features,out_features,lr = 1):\n",
        "        super(equalized_linear, self).__init__()\n",
        "        \n",
        "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "        torch.nn.init.normal_(self.weight.data, mean=0.0, std=1/lr)\n",
        "        self.bias = nn.Parameter(torch.zeros(out_features))\n",
        "        self.weight_scaler = lr / (in_features ** 0.5)\n",
        "        self.lr = lr\n",
        "\n",
        "    def forward(self,x,gain = np.sqrt(2)):  \n",
        "        return F.linear(x, self.weight * self.weight_scaler * gain, self.bias * self.lr)\n",
        "        \n",
        "\n",
        "class modulated_conv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros',mod = True,demod = True,style_dimension = 256):\n",
        "        super(modulated_conv2d, self).__init__()\n",
        "        self.padding, self.stride = padding, stride\n",
        "        lr = 1\n",
        "        self.weight = nn.Parameter(torch.randn(out_channels, in_channels, kernel_size, kernel_size))\n",
        "        torch.nn.init.normal_(self.weight.data, mean=0.0, std=1.0 / lr)\n",
        "        #self.weight = nn.Parameter(torch.zeros(out_channels, in_channels, kernel_size, kernel_size))\n",
        "        self.bias = nn.Parameter(torch.zeros(1,out_channels,1,1))\n",
        "        self.weight_scaler = lr / (in_channels * kernel_size*kernel_size)**0.5\n",
        "        self.mod = mod\n",
        "        self.demod = demod\n",
        "        if mod:\n",
        "            self.affine = equalized_linear(style_dimension, in_channels)\n",
        "      \n",
        "\n",
        "    def forward(self,x,style=None,shape = None,web = False,style_2 = None,gain = np.sqrt(2)):\n",
        "        oC, iC, kH, kW = self.weight.shape\n",
        "        if not web:#shape is None:\n",
        "            N, iC, H, W = x.shape\n",
        "        else:\n",
        "            N, iC, H, W = shape\n",
        "        \n",
        "        if not self.mod:\n",
        "            weight = self.weight.view(1,oC,iC,kH,kW) * self.weight_scaler\n",
        "            if not web:\n",
        "                weight = weight.expand(N,oC,iC,kH,kW)\n",
        "            \n",
        "            x = F.conv2d(x.view(1,N*iC,H,W),gain * weight.reshape(N*oC,iC,kH,kW),\n",
        "                    padding=self.padding, stride=self.stride, groups=N)\n",
        "            if web:\n",
        "                return x.view(N,oC,H,W) + self.bias\n",
        "            \n",
        "            return x.view(N,oC,x.shape[2],x.shape[3]) + self.bias\n",
        "        \n",
        "        affined_style = self.affine(style,gain=1) + 1\n",
        "\n",
        "        if web:\n",
        "            affined_style_2 = self.affine(style_2,gain=1) + 1\n",
        "            modulated_weight = self.weight.view(1,oC,iC,kH,kW) * self.weight_scaler\n",
        "            #modulated_weight = modulated_weight.repeat(N,1,1,1,1)\n",
        "            x = x * affined_style.view(N,iC,1,1)\n",
        "            x = F.conv2d(x.view(1,N*iC,H,W), gain * modulated_weight.view(N*oC,iC,kH,kW),\n",
        "                    padding=self.padding, stride=self.stride, groups=N).view(N,oC,H,W)\n",
        "            modulated_weight = modulated_weight * affined_style_2.view(N,1,iC,1,1)\n",
        "            demod_norm = 1\n",
        "            if self.demod:\n",
        "                demod_norm = 1 / torch.sqrt((modulated_weight * modulated_weight).sum([2,3,4])  + 1e-8)\n",
        "                out = x * demod_norm.view(N, oC, 1, 1) + self.bias\n",
        "                return out\n",
        "            out = x + self.bias\n",
        "\n",
        "        else:\n",
        "            modulated_weight = self.weight_scaler *self.weight.view(1,oC,iC,kH,kW) * affined_style.view(N,1,iC,1,1) \n",
        "\n",
        "            demod_norm = 1 / torch.sqrt((modulated_weight * modulated_weight).sum([2,3,4]) + 1e-8) # (N, oC)\n",
        "            demodulated_weight = modulated_weight\n",
        "            if self.demod:\n",
        "                demodulated_weight = demodulated_weight * demod_norm.view(N, oC, 1, 1, 1) \n",
        "           \n",
        "            out = F.conv2d(x.view(1,N*iC,H,W), gain * demodulated_weight.view(N*oC,iC,kH,kW),\n",
        "                    padding=self.padding, stride=self.stride, groups=N).view(N,oC,H,W) + self.bias\n",
        "            \n",
        "        return out\n",
        "\n",
        "def alternative_Upsample(image,input_size):\n",
        "    \n",
        "    batches, channels, h, w = input_size\n",
        "\n",
        "    x = image.view(batches, channels, h * w, 1)\n",
        "    x = torch.cat((x,x),3)\n",
        "    x = x.view(batches, channels, h, w * 2)\n",
        "    x = torch.cat((x,x),3)\n",
        "    x = x.view(batches, channels, h * 2, w * 2)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "class block(nn.Module):\n",
        "    def __init__(self,resolution,in_channels, mid_channels, out_channels,style_dimension = 512):\n",
        "        super(block,self).__init__()\n",
        "        self.conv_1 = modulated_conv2d(in_channels,mid_channels,kernel_size=3,stride=1,padding=1,style_dimension = style_dimension)\n",
        "        self.const_noise_1 = torch.randn((1,1,resolution,resolution),requires_grad=False, device = device)\n",
        "        self.noise_scalar_1 = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "        self.conv_2 = modulated_conv2d(mid_channels,out_channels,kernel_size=3,stride=1,padding=1,style_dimension = style_dimension)\n",
        "        self.const_noise_2 = torch.randn((1,1,resolution,resolution),requires_grad=False, device = device)\n",
        "        self.noise_scalar_2 = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "        self.skip = modulated_conv2d(in_channels,out_channels,kernel_size=1,stride=1,padding=0,mod = False)\n",
        "        self.resolution = resolution\n",
        "    def forward(self,x,style=None,web = False,style_2 = None):\n",
        "        t = x\n",
        "        x = self.conv_1(x,style,shape = (1,-1,self.resolution,self.resolution),style_2 = style_2,web = web)\n",
        "        x = F.leaky_relu(x,LReLU_alpha)\n",
        "        x = x + self.const_noise_1 * self.noise_scalar_1\n",
        "\n",
        "        x = self.conv_2(x,style,shape = (1,-1,self.resolution,self.resolution),style_2 = style_2,web = web)\n",
        "        x = F.leaky_relu(x,LReLU_alpha)\n",
        "        x = x + self.const_noise_2 * self.noise_scalar_2\n",
        "\n",
        "        #x = (x + self.skip(t,shape = (1,-1,self.resolution,self.resolution), web = web, gain = 1)) * (1 / math.sqrt(2))\n",
        "        return x\n",
        "\n",
        "# Define model\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        dimensions = [512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 256, 256, 128, 128, 64, 64, 32, 32, 16]\n",
        "        #[512, 512, 512, 256, 256, 128, 128, 64, 64, 32, 32, 16, 16, 8, 8, 4, 4, 2, 2]\n",
        "        #[256,256,256,256,128,128,128,128,128,128,128\n",
        "        #             ,64,64,32,32, 16, 16, 8, 8] \n",
        "        self.learning_const = nn.Parameter(torch.randn(1,dimensions[1],4,4))#torch.ones((1,dimensions[1],4,4),requires_grad=True, device = device)\n",
        "        self.mapping_network = self.generate_mapping_network(dimensions[0],8)\n",
        "\n",
        "        \n",
        "        self.block_4 = block(4,dimensions[1],dimensions[2],dimensions[3],dimensions[0])\n",
        "        self.to_rgb_4 = modulated_conv2d(dimensions[3],3,kernel_size=1,stride=1,padding=0,demod = False,style_dimension = dimensions[0])\n",
        "\n",
        "        self.block_8 = block(8,dimensions[3],dimensions[4],dimensions[5],dimensions[0])\n",
        "        self.to_rgb_8 = modulated_conv2d(dimensions[5],3,kernel_size=1,stride=1,padding=0,demod = False,style_dimension = dimensions[0])\n",
        "\n",
        "        self.block_16 = block(16,dimensions[5],dimensions[6],dimensions[7],dimensions[0])\n",
        "        self.to_rgb_16 = modulated_conv2d(dimensions[7],3,kernel_size=1,stride=1,padding=0,demod = False,style_dimension = dimensions[0])\n",
        "\n",
        "        self.block_32 = block(32,dimensions[7],dimensions[8],dimensions[9],dimensions[0])\n",
        "        self.to_rgb_32 = modulated_conv2d(dimensions[9],3,kernel_size=1,stride=1,padding=0,demod = False,style_dimension = dimensions[0])\n",
        "\n",
        "        self.block_64 = block(64,dimensions[9],dimensions[10],dimensions[11],dimensions[0])\n",
        "        self.to_rgb_64 = modulated_conv2d(dimensions[11],3,kernel_size=1,stride=1,padding=0,demod = False,style_dimension = dimensions[0])\n",
        "\n",
        "        self.block_128 = block(128,dimensions[11],dimensions[12],dimensions[13],dimensions[0])\n",
        "        self.to_rgb_128 = modulated_conv2d(dimensions[13],3,kernel_size=1,stride=1,padding=0,demod = False,style_dimension = dimensions[0])\n",
        "\n",
        "        self.block_256 = block(256,dimensions[13],dimensions[14],dimensions[15],dimensions[0])\n",
        "        self.to_rgb_256 = modulated_conv2d(dimensions[15],3,kernel_size=1,stride=1,padding=0,demod = False,style_dimension = dimensions[0])\n",
        "\n",
        "        self.block_512 = block(512,dimensions[15],dimensions[16],dimensions[17],dimensions[0])\n",
        "        self.to_rgb_512 = modulated_conv2d(dimensions[17],3,kernel_size=1,stride=1,padding=0,demod = False,style_dimension = dimensions[0])\n",
        "\n",
        "\n",
        "    \n",
        "    def generate_mapping_network(self,dimension = 512,number_of_layer = 8):\n",
        "       mapping_network = nn.Sequential()\n",
        "       for i in range(number_of_layer):\n",
        "           mapping_network.add_module('mapping_fc{0}'.format(i), equalized_linear(dimension,dimension,lr = 0.01))\n",
        "           mapping_network.add_module('mapping_lrelu{0}'.format(i), nn.LeakyReLU(LReLU_alpha))\n",
        "       return mapping_network\n",
        "\n",
        "    def forward(self, z, stage = 1 ,alpha = 0, batches = 1,web = False):\n",
        "        style = self.mapping_network(z)\n",
        "        style_2 = None\n",
        "        if web:\n",
        "            style_2 = self.mapping_network(z)\n",
        "            x = self.learning_const\n",
        "        else:\n",
        "            x = self.learning_const.repeat(int(batches),1,1,1)\n",
        "        for i in range(stage):\n",
        "            if i != 0:\n",
        "                if web:\n",
        "                    x = alternative_Upsample(x,(1,-1,2 ** (i + 1),2 ** (i + 1)))\n",
        "                else:\n",
        "                    x = F.interpolate(x,scale_factor=2, mode='nearest')\n",
        "\n",
        "            x = getattr(self, 'block_{0}'.format(2 ** (i + 2)))(x,style=style,web = web,style_2 = style_2)\n",
        "\n",
        "            if i == 0:\n",
        "                if web:\n",
        "                    x_out = getattr(self, 'to_rgb_{0}'.format(2 ** (i + 2)))(x,style = style,shape = (1,-1,int(2 ** (i + 2)),int(2 ** (i + 2))),web = True,style_2 = style_2,gain = np.sqrt(2))\n",
        "                else:\n",
        "                    x_out = getattr(self, 'to_rgb_{0}'.format(2 ** (i + 2)))(x,style = style,web = False,style_2 = style_2,gain = np.sqrt(2)) \n",
        "            else:\n",
        "                if web:\n",
        "                    x_out_2 = getattr(self, 'to_rgb_{0}'.format(2 ** (i + 2)))(x,style = style,shape = (1,-1,int(2 ** (i + 2)),int(2 ** (i + 2))),web = True,style_2 = style_2,gain = np.sqrt(2))\n",
        "                else:\n",
        "                    x_out_2 = getattr(self, 'to_rgb_{0}'.format(2 ** (i + 2)))(x,style = style, web = False,style_2 = style_2,gain = np.sqrt(2)) \n",
        "                x_out_2 = F.leaky_relu(x_out_2,LReLU_alpha)\n",
        "                if web:\n",
        "                    x_out = alternative_Upsample(x_out,(1,-1,2 ** (i + 1),2 ** (i + 1))) + x_out_2\n",
        "                else:\n",
        "                    x_out = F.interpolate(x_out,scale_factor=2, mode='nearest') + x_out_2\n",
        "\n",
        "            #x_out = getattr(self, 'to_rgb_{0}'.format(2 ** (stage + 1)))(x,web = False,gain = 1) \n",
        "                    \n",
        "        return x_out"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dl4fVrUadKk4"
      },
      "source": [
        "class d_block(nn.Module):\n",
        "    def __init__(self,in_channels, mid_channels, out_channels):\n",
        "        super(d_block,self).__init__()\n",
        "        self.conv_1 = modulated_conv2d(in_channels,mid_channels,kernel_size=3,stride=1,padding=1,mod = False)\n",
        "        self.conv_2 = modulated_conv2d(mid_channels,out_channels,kernel_size=3,stride=1,padding=1,mod = False)\n",
        "        self.skip =  modulated_conv2d(in_channels,out_channels,kernel_size=1,stride=1,padding=0,mod = False)\n",
        "\n",
        "    def forward(self,input_x):\n",
        "        x = self.conv_1(input_x)\n",
        "        x = F.leaky_relu(x,LReLU_alpha)\n",
        "        x = self.conv_2(x)\n",
        "        x = F.leaky_relu(x,LReLU_alpha)\n",
        "        x = (x + self.skip(input_x,gain = 1)) * (1 / np.sqrt(2))\n",
        "        return x\n",
        "  \n",
        "# Define model\n",
        "from torch.autograd import Variable\n",
        "from torch import autograd\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        dimensions = [512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 256, 256, 128, 128, 64, 64, 32, 32, 16]\n",
        "                     #,64,64,32,32, 16, 16, 8, 8] #[256,256,256,128,128,128,64,64,64,32,32,32]\n",
        "\n",
        "        self.from_rgb_4 = modulated_conv2d(3,dimensions[2],kernel_size=1,stride=1,padding=0,mod = False)\n",
        "        self.block_4 = d_block(dimensions[2],dimensions[1],dimensions[0])\n",
        "\n",
        "        self.from_rgb_8 = modulated_conv2d(3,dimensions[4],kernel_size=1,stride=1,padding=0,mod = False)\n",
        "        self.block_8 = d_block(dimensions[4],dimensions[3],dimensions[2])\n",
        "\n",
        "        self.from_rgb_16 = modulated_conv2d(3,dimensions[6],kernel_size=1,stride=1,padding=0,mod = False)\n",
        "        self.block_16 = d_block(dimensions[6],dimensions[5],dimensions[4])\n",
        "\n",
        "        self.from_rgb_32 = modulated_conv2d(3,dimensions[8],kernel_size=1,stride=1,padding=0,mod = False)\n",
        "        self.block_32 = d_block(dimensions[8],dimensions[7],dimensions[6])\n",
        "\n",
        "        self.from_rgb_64 = modulated_conv2d(3,dimensions[10],kernel_size=1,stride=1,padding=0,mod = False)\n",
        "        self.block_64 = d_block(dimensions[10],dimensions[9],dimensions[8])\n",
        "\n",
        "        self.from_rgb_128 = modulated_conv2d(3,dimensions[12],kernel_size=1,stride=1,padding=0,mod = False)\n",
        "        self.block_128 = d_block(dimensions[12],dimensions[11],dimensions[10])\n",
        "\n",
        "        self.from_rgb_256 = modulated_conv2d(3,dimensions[14],kernel_size=1,stride=1,padding=0,mod = False)\n",
        "        self.block_256 = d_block(dimensions[14],dimensions[13],dimensions[12])\n",
        "\n",
        "        self.from_rgb_512 = modulated_conv2d(3,dimensions[16],kernel_size=1,stride=1,padding=0,mod = False)\n",
        "        self.block_512 = d_block(dimensions[16],dimensions[15],dimensions[14])\n",
        "\n",
        "        self.final_conv_1 = modulated_conv2d(dimensions[0] + 1,dimensions[0],kernel_size=3,stride=1,padding=1,mod = False)\n",
        "        self.final_conv_2 = modulated_conv2d(dimensions[0],dimensions[0],kernel_size=4,stride=1,padding=0,mod = False)\n",
        "        self.linear_1 = equalized_linear(dimensions[0],dimensions[0])\n",
        "        self.linear_2 = equalized_linear(dimensions[0],1)\n",
        "\n",
        "    #https://github.com/Zeleni9/pytorch-wgan/blob/master/models/wgan_gradient_penalty.py を改変\n",
        "    def calculate_gradient_penalty(self, real_images, fake_images,batch_size,stage,alpha):\n",
        "        #eta = torch.FloatTensor(batch_size,1,1,1).uniform_(0,1).to(device)\n",
        "        #eta = eta.expand(batch_size, real_images.size(1), real_images.size(2), real_images.size(3))\n",
        "\n",
        "        interpolated = real_images#eta * real_images + ((1 - eta) * fake_images)\n",
        "\n",
        "        # define it to calculate gradient\n",
        "        interpolated = Variable(interpolated, requires_grad=True)\n",
        "        # calculate probability of interpolated examples\n",
        "        prob_interpolated = self(interpolated,stage,alpha,batch_size).sum()\n",
        "        # calculate gradients of probabilities with respect to examples\n",
        "        gradients = autograd.grad(outputs=prob_interpolated, inputs=interpolated,\n",
        "                              grad_outputs=torch.ones(\n",
        "                                  prob_interpolated.size()).to(device),\n",
        "                              create_graph=True, retain_graph=True)[0]\n",
        "                              \n",
        "        grad_penalty = ((gradients** 2).sum(axis=[1,2,3])).mean() #(gradients.norm(2, dim=1) ** 2).mean()\n",
        "        #grad_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
        "        return grad_penalty\n",
        "    \n",
        "    def forward(self, image, stage = 1 ,alpha = 0, batches = 1):\n",
        "        for i in range(stage):\n",
        "            image = F.interpolate(image,size=int(2 ** (stage + 1 - i)), mode='bilinear')\n",
        "            \n",
        "            if i == 0:\n",
        "                x =  F.leaky_relu(getattr(self, 'from_rgb_{0}'.format(2 ** (stage + 1 - i)))(image,gain=math.sqrt(2)),LReLU_alpha)\n",
        "            #else:\n",
        "            #    x = x + F.leaky_relu(getattr(self, 'from_rgb_{0}'.format(2 ** (stage + 1 - i)))(image,gain=1),LReLU_alpha)\n",
        "            \n",
        "            x = getattr(self, 'block_{0}'.format(2 ** (stage + 1 - i)))(x)\n",
        "            if i != stage - 1:\n",
        "                x = F.interpolate(x,scale_factor=0.5, mode='bilinear')\n",
        "        \n",
        "        minibatch_std = torch.std(x , dim=(0,1))\n",
        "        x = torch.cat((x,minibatch_std.broadcast_to(batches,1,4,4)),dim = 1)\n",
        "        x = self.final_conv_1(x)\n",
        "        x = F.leaky_relu(x,LReLU_alpha)\n",
        "        x = self.final_conv_2(x)\n",
        "        x = F.leaky_relu(x,LReLU_alpha)\n",
        "        x = torch.flatten(x,start_dim = 1)\n",
        "        x = self.linear_1(x)\n",
        "        x = F.leaky_relu(x,LReLU_alpha)\n",
        "        return  self.linear_2(x,gain = 1)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evars0QQOPuM"
      },
      "source": [
        "import copy\n",
        "\n",
        "def transform_for_batch(image,transforms):\n",
        "    #return transforms((image + 1) / 2) * 2 - 1\n",
        "    for i in range(len(image)):\n",
        "        tmp = image[i].reshape(1,image.shape[1],image.shape[2],image.shape[3])\n",
        "        image[i] = transforms((tmp + 1) / 2) * 2 - 1\n",
        "    return image#transforms((image + 1) / 2) * 2 - 1\n",
        "\n",
        "class trainer():\n",
        "    def __init__(self,learning_rate = 0.002, stage = 4,BATCH_SIZE = 64, n_critic = 1,gp_lambda = 10,z_dimension = 256,gs_beta = 0.999,is_16bit = True):\n",
        "        self.stage = stage\n",
        "        self.BATCH_SIZE = BATCH_SIZE\n",
        "        self.n_critic = n_critic\n",
        "        self.gp_lambda = gp_lambda\n",
        "        self.g = Generator().to(device)\n",
        "        self.gs = copy.deepcopy(self.g)\n",
        "        self.d = Discriminator().to(device)\n",
        "        self.G_optimizer = torch.optim.Adam(self.g.parameters(), lr=learning_rate, betas=(0, 0.99))\n",
        "        self.D_optimizer = torch.optim.Adam(self.d.parameters(), lr=learning_rate, betas=(0, 0.99))\n",
        "        self.z_dimension = z_dimension\n",
        "        self.gs_beta = gs_beta\n",
        "        self.noise = torch.randn((self.BATCH_SIZE,self.z_dimension),device = device)\n",
        "        self.is_16bit = is_16bit\n",
        "        self.num_trained_images = 0\n",
        "    def train(self):\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "        '''\n",
        "        training_data = datasets.CIFAR10(\n",
        "            root=\"data\",\n",
        "            train=True,\n",
        "            download=True,\n",
        "            transform=ToTensor(),\n",
        "        )\n",
        "        '''\n",
        "\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((2 * 2 ** self.stage , 2 * 2 ** self.stage)),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            #transforms.RandomResizedCrop(2 * 2 ** self.stage, scale=(0.8, 1.0), ratio=(3 / 4, 4 / 3)),\n",
        "            #transforms.RandomCrop(2 * 2 ** self.stage , padding= 2 ** (self.stage - 2), pad_if_needed=True, padding_mode='reflect'),\n",
        "            #transforms.ColorJitter(brightness=0.2, contrast=0, saturation=0),\n",
        "            transforms.RandomErasing(),\n",
        "        ])\n",
        "\n",
        "        training_data = CustomImageDataset(\n",
        "                    img_dir=\"/content/dataset/\",\n",
        "                    transforms = transforms.Resize((2 * 2 ** self.stage , 2 * 2 ** self.stage))\n",
        "                )\n",
        "        \n",
        "        train_dataloader = DataLoader(training_data, batch_size=self.BATCH_SIZE, shuffle=True, drop_last=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "        if self.is_16bit:\n",
        "            scaler_D = torch.cuda.amp.GradScaler()\n",
        "            scaler_G = torch.cuda.amp.GradScaler()\n",
        "        while self.num_trained_images < 128000:\n",
        "            for X in train_dataloader:\n",
        "                X = X#[0]\n",
        "                \n",
        "                for _ in range(self.n_critic):   \n",
        "                    #self.D_optimizer.zero_grad()\n",
        "                    for param in self.d.parameters():\n",
        "                        param.grad = None\n",
        "                    if self.is_16bit:\n",
        "                    #with torch.cuda.amp.autocast(): \n",
        "                        torch.cuda.amp.autocast(True)     \n",
        "\n",
        "                    z = torch.randn((self.BATCH_SIZE,self.z_dimension),device = device)\n",
        "                    generated = self.g(z, stage = self.stage ,alpha = 0, batches = self.BATCH_SIZE)\n",
        "                    generated = transform_for_batch(generated, transform)    \n",
        "\n",
        "                    X = X.to(device)\n",
        "                    X = X * 2 - 1\n",
        "                    X = transform_for_batch(X, transform)\n",
        "                    #X = torch.nn.functional.interpolate(X,size=generated.size(2), mode='bilinear')\n",
        "                    \n",
        "                    y_real = self.d(X, stage = self.stage ,alpha = 0, batches = self.BATCH_SIZE)\n",
        "                    y_fake = self.d(generated.data, stage = self.stage ,alpha = 0, batches = self.BATCH_SIZE)\n",
        "                    #d_loss = torch.mean(y_fake) - torch.mean(y_real)\n",
        "                    d_loss = torch.mean(F.softplus(y_fake) + F.softplus(-y_real))#- torch.mean(torch.log(torch.sigmoid(y_real))) - torch.mean(torch.log(1 - torch.sigmoid(y_fake)))#\n",
        "                    pure_loss = d_loss.data\n",
        "                    d_loss = d_loss + (self.d.calculate_gradient_penalty(X,generated,self.BATCH_SIZE,self.stage,alpha = 0) * self.gp_lambda)\n",
        "\n",
        "                    if self.is_16bit:\n",
        "                        torch.cuda.amp.autocast(False)     \n",
        "\n",
        "\n",
        "                    if self.is_16bit:\n",
        "                        scaler_D.scale(d_loss).backward()\n",
        "                        scaler_D.step(self.D_optimizer)\n",
        "                        scaler_D.update()\n",
        "                    else:\n",
        "                        d_loss.backward()\n",
        "                        self.D_optimizer.step()\n",
        "                   \n",
        "                for param in self.g.parameters():\n",
        "                        param.grad = None\n",
        "                if self.is_16bit:\n",
        "                #with torch.cuda.amp.autocast():\n",
        "                    torch.cuda.amp.autocast(True)\n",
        "\n",
        "                z = torch.randn((self.BATCH_SIZE,self.z_dimension),device = device)\n",
        "                generated = self.g(z, stage = self.stage ,alpha = 0, batches = self.BATCH_SIZE)\n",
        "                generated = transform_for_batch(generated, transform)\n",
        "                y_fake = self.d(generated, stage = self.stage ,alpha = 0, batches = self.BATCH_SIZE)\n",
        "                #g_loss = -torch.mean(y_fake)\n",
        "                g_loss = torch.mean(F.softplus(-y_fake)) #torch.mean(torch.log(1 - torch.sigmoid(y_fake)))#\n",
        "                \n",
        "                if self.is_16bit:\n",
        "                    torch.cuda.amp.autocast(False)\n",
        "                '''#normal step\n",
        "                g_loss.backward()\n",
        "                self.G_optimizer.step()\n",
        "                '''\n",
        "\n",
        "                if self.is_16bit:\n",
        "                    scaler_G.scale(g_loss).backward()\n",
        "                    scaler_G.step(self.G_optimizer)\n",
        "                    scaler_G.update()\n",
        "                else:\n",
        "                    g_loss.backward()\n",
        "                    self.G_optimizer.step()\n",
        "\n",
        "                for gparam, gsparam in zip(self.g.parameters(), self.gs.parameters()):\n",
        "                    gsparam.data = (1 - self.gs_beta) * gsparam.data + self.gs_beta * gparam.data\n",
        "\n",
        "                self.num_trained_images += self.BATCH_SIZE\n",
        "                if self.num_trained_images // 1000 != (self.num_trained_images - self.BATCH_SIZE) // 1000:\n",
        "                    print('stage:{},trained images:{},g_loss:{}, d_loss:{}, Pure discriminator Loss:{}'.format(self.stage,self.num_trained_images,g_loss,d_loss,pure_loss))\n",
        "                    generated = self.gs(self.noise, stage = self.stage ,alpha = 0, batches = self.noise.shape[0])\n",
        "                    #generated = transform_for_batch(generated, transform)    \n",
        "                    #generated = F.interpolate(generated,size=512, mode='nearest')\n",
        "                    show_image(generated,str(self.num_trained_images))\n",
        "                    show_image(X,'augmented_sample')\n",
        "                    torch.save(self, '/content/drive/MyDrive/StyleGAN2/modelCGG-scratch-128-quality-highchannel.pth')\n",
        "                "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEtoDy1JBkio",
        "outputId": "b0d26665-eb73-4035-ced5-aa7c3d6fa76e"
      },
      "source": [
        "try:\n",
        "    train = torch.load('/content/drive/MyDrive/StyleGAN2/modelCGG-scratch-128-quality-highchannel.pth')\n",
        "    print('loaded')\n",
        "except:\n",
        "    train = trainer(learning_rate = 0.002, stage = 8,BATCH_SIZE = 4, n_critic = 1,gp_lambda = 5,z_dimension = 512,gs_beta = 0.999,is_16bit = False)\n",
        "    print('load failed')\n",
        " \n",
        "#stage:resolution\n",
        "#1:4, 2:8, 3:16, 4:32, 5:64, 6:128, 7:256, 8:512\n",
        "BATCH_SIZE = 4\n",
        "train.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3613: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3658: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "stage:8,trained images:57000,g_loss:2.7278828620910645, d_loss:0.5194001197814941, Pure discriminator Loss:0.4432961344718933\n",
            "stage:8,trained images:58000,g_loss:2.121715784072876, d_loss:1.1676274538040161, Pure discriminator Loss:1.020789384841919\n",
            "stage:8,trained images:59000,g_loss:6.455488204956055, d_loss:0.7375357151031494, Pure discriminator Loss:0.6752704381942749\n",
            "stage:8,trained images:60000,g_loss:0.6790080070495605, d_loss:1.4580622911453247, Pure discriminator Loss:1.267101764678955\n",
            "stage:8,trained images:61000,g_loss:4.950351715087891, d_loss:0.601292610168457, Pure discriminator Loss:0.5676662921905518\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMy29H2BzU5w"
      },
      "source": [
        "#make ONNX model\n",
        "'''\n",
        "try:\n",
        "    train = torch.load('/content/drive/MyDrive/StyleGAN2/model128-cifar.pth')\n",
        "except:\n",
        "    train = trainer()\n",
        "    print('load failed')\n",
        "\n",
        "dummy_input = torch.randn(1, train.z_dimension, device=device)\n",
        "train.gs.learning_const.requires_grad = False\n",
        "for i in range(5):  \n",
        "    getattr(train.gs, 'block_{0}'.format(2 ** (i + 2))).const_noise_1.requires_grad = False\n",
        "    getattr(train.gs, 'block_{0}'.format(2 ** (i + 2))).const_noise_2.requires_grad = False\n",
        "stage = torch.tensor(4, dtype=torch.int)\n",
        "alpha = torch.tensor(0, dtype=torch.int)\n",
        "batches = torch.tensor(1, dtype=torch.int)\n",
        "web = torch.tensor(True, dtype=torch.bool)\n",
        "torch.onnx.export(train.gs, (dummy_input,stage,alpha,batches,web), 'generator.onnx', opset_version= 9)\n",
        "\n",
        "from google.colab import files\n",
        "files.download('generator.onnx')\n",
        "'''\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMqPXkseJ6xc"
      },
      "source": [
        "#Test\n",
        "'''\n",
        "train = torch.load('/content/drive/MyDrive/StyleGAN2/model128-cifar4.pth')\n",
        "batch_size = 1\n",
        "z_dimension = 128\n",
        "noise = torch.ones((batch_size,z_dimension),device = device)\n",
        "generated = train.gs(noise, stage = 4 ,alpha = 0, batches = batch_size,web =True)\n",
        "show_image(generated,str('test'))\n",
        "print(generated)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTj3F0l29vVP"
      },
      "source": [
        "!rm -r fake_images\n",
        "!rm -r real_images\n",
        "!mkdir fake_images\n",
        "!mkdir real_images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CLl5bNI4mDH"
      },
      "source": [
        "#Evaluating image quality by FID\n",
        "import random\n",
        "try:\n",
        "    train = torch.load('/content/drive/MyDrive/StyleGAN2/modelCGG-scratch-128-quality-highchannel.pth')\n",
        "except:\n",
        "    train = trainer()\n",
        "    print('load failed')\n",
        "\n",
        "number_of_images =  16384\n",
        "batch_size = 32\n",
        "z_dimension = 512\n",
        "\n",
        "\n",
        "for i in range(int(number_of_images / batch_size)):\n",
        "    noise = torch.randn((batch_size,z_dimension),device = device)\n",
        "    generated = train.gs(noise, stage = train.stage ,alpha = 0, batches = batch_size)\n",
        "\n",
        "    generated_image = generated.to('cpu').detach().numpy().copy()\n",
        "    generated_image = generated_image * 127.5 + 127.5\n",
        "    generated_image = np.where(generated_image < 0, 0, generated_image)\n",
        "    generated_image = np.where(generated_image > 255, 255, generated_image)\n",
        "    generated_image=np.transpose(generated_image, (0, 2, 3, 1))\n",
        "    generated_image = generated_image.astype(np.uint8)\n",
        "\n",
        "    for j, image in enumerate(generated_image):\n",
        "        Image.fromarray(image).save('./fake_images/' + str(i * batch_size + j) + '.png')\n",
        "\n",
        "\n",
        "'''\n",
        "training_data = datasets.CIFAR10(\n",
        "            root=\"data\",\n",
        "            train=True,\n",
        "            download=True,\n",
        "            transform=ToTensor(),\n",
        ")\n",
        "'''\n",
        "training_data = CustomImageDataset(\n",
        "img_dir=\"/content/dataset/\",\n",
        "transforms = transforms.Resize((2 * 2 ** train.stage , 2 * 2 ** train.stage))\n",
        ")\n",
        "\n",
        "for i in range(len(training_data)):\n",
        "    image = training_data.__getitem__(i,size = 2 * 2 ** train.stage)#[0]\n",
        "    image = image * 255\n",
        "    image=np.transpose(image, (1, 2, 0))\n",
        "    image = image.detach().numpy().copy().astype(np.uint8)\n",
        "    Image.fromarray(image).save('./real_images/' + str(i) + '.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vyYJN-zDB9Y"
      },
      "source": [
        "!pip install pytorch-fid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFEU6o7ADR8j"
      },
      "source": [
        "!python -m pytorch_fid ./real_images ./fake_images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fG1xUukI5JZQ"
      },
      "source": [
        "display_png(Image.open('./fake_images/0.png'))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}